{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb230474-f8b7-49b6-bbba-0a0aa6d88b03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting gdown\n  Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\nRequirement already satisfied: requests[socks] in /databricks/python3/lib/python3.9/site-packages (from gdown) (2.27.1)\nRequirement already satisfied: beautifulsoup4 in /databricks/python3/lib/python3.9/site-packages (from gdown) (4.11.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown) (3.9.0)\nCollecting tqdm\n  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\nRequirement already satisfied: soupsieve>1.2 in /databricks/python3/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.3.1)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests[socks]->gdown) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests[socks]->gdown) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests[socks]->gdown) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests[socks]->gdown) (1.26.9)\nCollecting PySocks!=1.5.7,>=1.5.6\n  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\nInstalling collected packages: PySocks, tqdm, gdown\nSuccessfully installed PySocks-1.7.1 gdown-5.2.0 tqdm-4.67.1\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install gdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f296e7a0-6410-4f94-981c-5c6e5a30d9f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1abe9EkM_uf2F2hjEkbhMBG9Mf2dFE4Wo CustomerImportance.csv\nProcessing file 1AGXVlDhbMbhoGXDJG0IThnqz86Qy3hqb transactions.csv\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mFileURLRetrievalError\u001B[0m                     Traceback (most recent call last)\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-17ae9c7b-8ceb-40e9-8641-d1f5e0cba173/lib/python3.9/site-packages/gdown/download.py:267\u001B[0m, in \u001B[0;36mdownload\u001B[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001B[0m\n",
       "\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 267\u001B[0m     url \u001B[38;5;241m=\u001B[39m \u001B[43mget_url_from_gdrive_confirmation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mres\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m FileURLRetrievalError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-17ae9c7b-8ceb-40e9-8641-d1f5e0cba173/lib/python3.9/site-packages/gdown/download.py:53\u001B[0m, in \u001B[0;36mget_url_from_gdrive_confirmation\u001B[0;34m(contents)\u001B[0m\n",
       "\u001B[1;32m     52\u001B[0m         error \u001B[38;5;241m=\u001B[39m m\u001B[38;5;241m.\u001B[39mgroups()[\u001B[38;5;241m0\u001B[39m]\n",
       "\u001B[0;32m---> 53\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m FileURLRetrievalError(error)\n",
       "\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m url:\n",
       "\n",
       "\u001B[0;31mFileURLRetrievalError\u001B[0m: Too many users have viewed or downloaded this file recently. Please try accessing the file again later. If the file you are trying to access is particularly large or is shared with many people, it may take up to 24 hours to be able to view or download the file. If you still can't access a file after 24 hours, contact your domain administrator.\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "\u001B[0;31mFileURLRetrievalError\u001B[0m                     Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2994928411265012>:7\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://drive.google.com/drive/folders/1qryhdlgNsmecWRy2haI8S3uC63wKk5X-\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m      5\u001B[0m output_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs/tmp/transactions\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# Where to save it\u001B[39;00m\n",
       "\u001B[0;32m----> 7\u001B[0m gdown\u001B[38;5;241m.\u001B[39mdownload_folder(url\u001B[38;5;241m=\u001B[39murl, output\u001B[38;5;241m=\u001B[39moutput_path, quiet\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, use_cookies\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-17ae9c7b-8ceb-40e9-8641-d1f5e0cba173/lib/python3.9/site-packages/gdown/download_folder.py:325\u001B[0m, in \u001B[0;36mdownload_folder\u001B[0;34m(url, id, output, quiet, proxy, speed, use_cookies, remaining_ok, verify, user_agent, skip_download, resume)\u001B[0m\n",
       "\u001B[1;32m    322\u001B[0m     files\u001B[38;5;241m.\u001B[39mappend(local_path)\n",
       "\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
       "\u001B[0;32m--> 325\u001B[0m local_path \u001B[38;5;241m=\u001B[39m \u001B[43mdownload\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    326\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhttps://drive.google.com/uc?id=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    327\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_path\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    328\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquiet\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquiet\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    329\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproxy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxy\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    330\u001B[0m \u001B[43m    \u001B[49m\u001B[43mspeed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspeed\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    331\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cookies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cookies\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    332\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverify\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverify\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    333\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    334\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    335\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m local_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    336\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m quiet:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-17ae9c7b-8ceb-40e9-8641-d1f5e0cba173/lib/python3.9/site-packages/gdown/download.py:278\u001B[0m, in \u001B[0;36mdownload\u001B[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001B[0m\n",
       "\u001B[1;32m    268\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m FileURLRetrievalError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    269\u001B[0m         message \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m    270\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to retrieve file url:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    271\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou may still be able to access the file from the browser:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    276\u001B[0m             url_origin,\n",
       "\u001B[1;32m    277\u001B[0m         )\n",
       "\u001B[0;32m--> 278\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m FileURLRetrievalError(message)\n",
       "\u001B[1;32m    280\u001B[0m filename_from_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    281\u001B[0m last_modified_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mFileURLRetrievalError\u001B[0m: Failed to retrieve file url:\n",
       "\n",
       "\tToo many users have viewed or downloaded this file recently. Please\n",
       "\ttry accessing the file again later. If the file you are trying to\n",
       "\taccess is particularly large or is shared with many people, it may\n",
       "\ttake up to 24 hours to be able to view or download the file. If you\n",
       "\tstill can't access a file after 24 hours, contact your domain\n",
       "\tadministrator.\n",
       "\n",
       "You may still be able to access the file from the browser:\n",
       "\n",
       "\thttps://drive.google.com/uc?id=1abe9EkM_uf2F2hjEkbhMBG9Mf2dFE4Wo\n",
       "\n",
       "but Gdown can't. Please check connections and permissions."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mFileURLRetrievalError\u001B[0m                     Traceback (most recent call last)\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-17ae9c7b-8ceb-40e9-8641-d1f5e0cba173/lib/python3.9/site-packages/gdown/download.py:267\u001B[0m, in \u001B[0;36mdownload\u001B[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001B[0m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 267\u001B[0m     url \u001B[38;5;241m=\u001B[39m \u001B[43mget_url_from_gdrive_confirmation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mres\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m FileURLRetrievalError \u001B[38;5;28;01mas\u001B[39;00m e:\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-17ae9c7b-8ceb-40e9-8641-d1f5e0cba173/lib/python3.9/site-packages/gdown/download.py:53\u001B[0m, in \u001B[0;36mget_url_from_gdrive_confirmation\u001B[0;34m(contents)\u001B[0m\n\u001B[1;32m     52\u001B[0m         error \u001B[38;5;241m=\u001B[39m m\u001B[38;5;241m.\u001B[39mgroups()[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m---> 53\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m FileURLRetrievalError(error)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m url:\n\n\u001B[0;31mFileURLRetrievalError\u001B[0m: Too many users have viewed or downloaded this file recently. Please try accessing the file again later. If the file you are trying to access is particularly large or is shared with many people, it may take up to 24 hours to be able to view or download the file. If you still can't access a file after 24 hours, contact your domain administrator.\n\nDuring handling of the above exception, another exception occurred:\n\n\u001B[0;31mFileURLRetrievalError\u001B[0m                     Traceback (most recent call last)\nFile \u001B[0;32m<command-2994928411265012>:7\u001B[0m\n\u001B[1;32m      4\u001B[0m url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://drive.google.com/drive/folders/1qryhdlgNsmecWRy2haI8S3uC63wKk5X-\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      5\u001B[0m output_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs/tmp/transactions\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# Where to save it\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m gdown\u001B[38;5;241m.\u001B[39mdownload_folder(url\u001B[38;5;241m=\u001B[39murl, output\u001B[38;5;241m=\u001B[39moutput_path, quiet\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, use_cookies\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-17ae9c7b-8ceb-40e9-8641-d1f5e0cba173/lib/python3.9/site-packages/gdown/download_folder.py:325\u001B[0m, in \u001B[0;36mdownload_folder\u001B[0;34m(url, id, output, quiet, proxy, speed, use_cookies, remaining_ok, verify, user_agent, skip_download, resume)\u001B[0m\n\u001B[1;32m    322\u001B[0m     files\u001B[38;5;241m.\u001B[39mappend(local_path)\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m--> 325\u001B[0m local_path \u001B[38;5;241m=\u001B[39m \u001B[43mdownload\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    326\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhttps://drive.google.com/uc?id=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    327\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    328\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquiet\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquiet\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    329\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproxy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    330\u001B[0m \u001B[43m    \u001B[49m\u001B[43mspeed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspeed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    331\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cookies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cookies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    332\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverify\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverify\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    333\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    334\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    335\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m local_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    336\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m quiet:\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-17ae9c7b-8ceb-40e9-8641-d1f5e0cba173/lib/python3.9/site-packages/gdown/download.py:278\u001B[0m, in \u001B[0;36mdownload\u001B[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001B[0m\n\u001B[1;32m    268\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m FileURLRetrievalError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    269\u001B[0m         message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    270\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to retrieve file url:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    271\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou may still be able to access the file from the browser:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    276\u001B[0m             url_origin,\n\u001B[1;32m    277\u001B[0m         )\n\u001B[0;32m--> 278\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m FileURLRetrievalError(message)\n\u001B[1;32m    280\u001B[0m filename_from_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    281\u001B[0m last_modified_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\n\u001B[0;31mFileURLRetrievalError\u001B[0m: Failed to retrieve file url:\n\n\tToo many users have viewed or downloaded this file recently. Please\n\ttry accessing the file again later. If the file you are trying to\n\taccess is particularly large or is shared with many people, it may\n\ttake up to 24 hours to be able to view or download the file. If you\n\tstill can't access a file after 24 hours, contact your domain\n\tadministrator.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=1abe9EkM_uf2F2hjEkbhMBG9Mf2dFE4Wo\n\nbut Gdown can't. Please check connections and permissions.",
       "errorSummary": "<span class='ansi-red-fg'>FileURLRetrievalError</span>: Failed to retrieve file url:\n\n\tToo many users have viewed or downloaded this file recently. Please\n\ttry accessing the file again later. If the file you are trying to\n\taccess is particularly large or is shared with many people, it may\n\ttake up to 24 hours to be able to view or download the file. If you\n\tstill can't access a file after 24 hours, contact your domain\n\tadministrator.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=1abe9EkM_uf2F2hjEkbhMBG9Mf2dFE4Wo\n\nbut Gdown can't. Please check connections and permissions.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importing data from GD\n",
    "import gdown\n",
    "\n",
    "url = \"https://drive.google.com/drive/folders/1qryhdlgNsmecWRy2haI8S3uC63wKk5X-\"\n",
    "output_path = \"/dbfs/tmp/transactions\"  # Where to save it\n",
    "\n",
    "gdown.download_folder(url=url, output=output_path, quiet=False, use_cookies=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c04a8c6-82ec-4af4-bd30-98a9170aa221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2994928411265014>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Move it to DBFS for Spark access\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mmv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile:/dbfs/tmp/transactions/transactions.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/tmp/transactions/transactions.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      3\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mmv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile:/dbfs/tmp/transactions/CustomerImportance.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/tmp/transactions/CustomerImportance.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:364\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    362\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    363\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 364\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
       "\n",
       "\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o444.mv.\n",
       ": java.io.FileNotFoundException: File file:/dbfs/tmp/transactions/transactions.csv does not exist\n",
       "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
       "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
       "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
       "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
       "\tat com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem.super$getFileStatus(WorkspaceLocalFileSystem.scala:75)\n",
       "\tat com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem.$anonfun$getFileStatus$1(WorkspaceLocalFileSystem.scala:75)\n",
       "\tat com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem.$anonfun$getFileStatus$1$adapted(WorkspaceLocalFileSystem.scala:74)\n",
       "\tat com.databricks.backend.daemon.driver.WSFSCredentialForwardingHelper.$anonfun$withWSFSCredentials$1(WorkspaceLocalFileSystem.scala:215)\n",
       "\tat com.databricks.backend.daemon.driver.WSFSCredentialForwardingHelper.withWSFSCredentials(WorkspaceLocalFileSystem.scala:186)\n",
       "\tat com.databricks.backend.daemon.driver.WSFSCredentialForwardingHelper.withWSFSCredentials(WorkspaceLocalFileSystem.scala:215)\n",
       "\tat com.databricks.backend.daemon.driver.WSFSCredentialForwardingHelper.withWSFSCredentials$(WorkspaceLocalFileSystem.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem.withWSFSCredentials(WorkspaceLocalFileSystem.scala:32)\n",
       "\tat com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem.getFileStatus(WorkspaceLocalFileSystem.scala:74)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withMvSafetyChecks$2(DBUtilsCore.scala:185)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:149)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withMvSafetyChecks$1(DBUtilsCore.scala:181)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:149)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withMvSafetyChecks(DBUtilsCore.scala:181)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$mv$2(DBUtilsCore.scala:390)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:144)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$mv$1(DBUtilsCore.scala:390)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:135)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.mv(DBUtilsCore.scala:389)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-2994928411265014>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Move it to DBFS for Spark access\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mmv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile:/dbfs/tmp/transactions/transactions.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/tmp/transactions/transactions.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mmv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile:/dbfs/tmp/transactions/CustomerImportance.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/tmp/transactions/CustomerImportance.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:364\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    362\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    363\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 364\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o444.mv.\n: java.io.FileNotFoundException: File file:/dbfs/tmp/transactions/transactions.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem.super$getFileStatus(WorkspaceLocalFileSystem.scala:75)\n\tat com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem.$anonfun$getFileStatus$1(WorkspaceLocalFileSystem.scala:75)\n\tat com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem.$anonfun$getFileStatus$1$adapted(WorkspaceLocalFileSystem.scala:74)\n\tat com.databricks.backend.daemon.driver.WSFSCredentialForwardingHelper.$anonfun$withWSFSCredentials$1(WorkspaceLocalFileSystem.scala:215)\n\tat com.databricks.backend.daemon.driver.WSFSCredentialForwardingHelper.withWSFSCredentials(WorkspaceLocalFileSystem.scala:186)\n\tat com.databricks.backend.daemon.driver.WSFSCredentialForwardingHelper.withWSFSCredentials(WorkspaceLocalFileSystem.scala:215)\n\tat com.databricks.backend.daemon.driver.WSFSCredentialForwardingHelper.withWSFSCredentials$(WorkspaceLocalFileSystem.scala:213)\n\tat com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem.withWSFSCredentials(WorkspaceLocalFileSystem.scala:32)\n\tat com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem.getFileStatus(WorkspaceLocalFileSystem.scala:74)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withMvSafetyChecks$2(DBUtilsCore.scala:185)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:149)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withMvSafetyChecks$1(DBUtilsCore.scala:181)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:149)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withMvSafetyChecks(DBUtilsCore.scala:181)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$mv$2(DBUtilsCore.scala:390)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:144)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$mv$1(DBUtilsCore.scala:390)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:71)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:135)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.mv(DBUtilsCore.scala:389)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "java.io.FileNotFoundException: File file:/dbfs/tmp/transactions/transactions.csv does not exist",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Move it to DBFS for Spark access\n",
    "dbutils.fs.mv(\"file:/dbfs/tmp/transactions/transactions.csv\", \"dbfs:/tmp/transactions/transactions.csv\")\n",
    "dbutils.fs.mv(\"file:/dbfs/tmp/transactions/CustomerImportance.csv\", \"dbfs:/tmp/transactions/CustomerImportance.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "144ed987-c0b7-485c-b31f-1b2551528b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n|step|     customer|age|gender|zipcodeOri|     merchant|zipMerchant|           category|amount|fraud|\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n|   0|'C1093826151'|'4'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'|  4.55|    0|\n|   0| 'C352968107'|'2'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 39.68|    0|\n|   0|'C2054744914'|'4'|   'F'|   '28007'|'M1823072687'|    '28007'|'es_transportation'| 26.89|    0|\n|   0|'C1760612790'|'3'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 17.25|    0|\n|   0| 'C757503768'|'5'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 35.72|    0|\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "#load transaction data\n",
    "df_txn = spark.read.csv(\"dbfs:/tmp/transactions/transactions.csv\", header=True, inferSchema=True)\n",
    "df_txn.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234b7acd-fbc8-4684-8fb5-9df551c64fb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+------+-------------------+-----+\n|       Source|       Target|Weight|          typeTrans|fraud|\n+-------------+-------------+------+-------------------+-----+\n|'C1093826151'| 'M348934600'|  4.55|'es_transportation'|    0|\n| 'C352968107'| 'M348934600'| 39.68|'es_transportation'|    0|\n|'C2054744914'|'M1823072687'| 26.89|'es_transportation'|    0|\n|'C1760612790'| 'M348934600'| 17.25|'es_transportation'|    0|\n| 'C757503768'| 'M348934600'| 35.72|'es_transportation'|    0|\n+-------------+-------------+------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "#load importance data\n",
    "df_importance = spark.read.csv(\"dbfs:/tmp/transactions/CustomerImportance.csv\", header=True, inferSchema=True)\n",
    "df_importance.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "603735e0-f5ed-48da-b457-9f085349470c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_txn = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"dbfs:/tmp/transactions/transactions.csv\")\n",
    "\n",
    "# Read CustomerImportance.csv\n",
    "df_importance_raw = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"dbfs:/tmp/transactions/CustomerImportance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96a6e591-1842-4834-a3d1-eb5002c9e707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_importance = df_importance_raw.selectExpr(\n",
    "    \"Source as customer\", \n",
    "    \"Target as merchant\", \n",
    "    \"typeTrans as category\", \n",
    "    \"Weight as weight\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74032c98-34ab-49bc-bbad-49c4be29caf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Testing the pattern identifications with the entire data to get an idea of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c9afad-a028-472a-821e-37e72dcc5c25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F, Window\n",
    "\n",
    "# Join transactions with importance weights\n",
    "# Join on customer, merchant, and transaction category\n",
    "df_joined = df_txn.join(df_importance, on=[\"customer\", \"merchant\", \"category\"], how=\"left\")\n",
    "\n",
    "\n",
    "# Total transactions per merchant\n",
    "merchant_txn_counts = df_txn.groupBy(\"merchant\").agg(F.count(\"*\").alias(\"merchant_txn_count\"))\n",
    "\n",
    "# Filter merchants with > 50k transactions\n",
    "large_merchants = merchant_txn_counts.filter(\"merchant_txn_count > 50000\")\n",
    "\n",
    "# Transactions for large merchants\n",
    "filtered_txns = df_joined.join(large_merchants, on=\"merchant\")\n",
    "\n",
    "# Transactions per customer per merchant\n",
    "customer_txn_count = filtered_txns.groupBy(\"merchant\", \"customer\").agg(\n",
    "    F.count(\"*\").alias(\"txn_count\"),\n",
    "    F.avg(\"weight\").alias(\"avg_weight\")\n",
    ")\n",
    "\n",
    "# Compute percentiles using approxQuantile (alternative: Window if needed)\n",
    "for merchant in large_merchants.select(\"merchant\").collect():\n",
    "    m = merchant[\"merchant\"]\n",
    "    df_m = customer_txn_count.filter(F.col(\"merchant\") == m)\n",
    "    txn_thresh = df_m.approxQuantile(\"txn_count\", [0.99], 0.01)[0]\n",
    "    weight_thresh = df_m.approxQuantile(\"avg_weight\", [0.01], 0.01)[0]\n",
    "    \n",
    "    df_detected = df_m.filter(\n",
    "        (F.col(\"txn_count\") >= txn_thresh) & (F.col(\"avg_weight\") <= weight_thresh)\n",
    "    ).withColumn(\"patternId\", F.lit(\"PatId1\")) \\\n",
    "     .withColumn(\"actionType\", F.lit(\"UPGRADE\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d49b46cf-02b3-47f8-a1b3-fa66237a8584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n|     merchant|merchant_txn_count|\n+-------------+------------------+\n| 'M348934600'|            205426|\n|'M1823072687'|            299693|\n+-------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "large_merchants.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4bd54db-580b-4177-be96-ff3ae9c97559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n|summary|         txn_count|        avg_amount|\n+-------+------------------+------------------+\n|  count|             47132|             47132|\n|   mean|12.616545022490028|106.61830155296524|\n| stddev| 29.13387597583581| 295.6577402990145|\n|    min|                 1|              0.01|\n|    max|               163|           8329.96|\n+-------+------------------+------------------+\n\nPatId2 count: 34\n+-------------+-------------+---------+------------------+---------+----------+-------------+-------------+\n|customer     |merchant     |txn_count|avg_amount        |patternId|actionType|customerName |merchantId   |\n+-------------+-------------+---------+------------------+---------+----------+-------------+-------------+\n|'C71938921'  |'M348934600' |112      |22.461785714285718|PatId2   |CHILD     |'C71938921'  |'M348934600' |\n|'C1799527037'|'M1823072687'|103      |22.75106796116505 |PatId2   |CHILD     |'C1799527037'|'M1823072687'|\n|'C1432312197'|'M348934600' |101      |22.729900990099008|PatId2   |CHILD     |'C1432312197'|'M348934600' |\n|'C245908628' |'M348934600' |125      |22.510800000000003|PatId2   |CHILD     |'C245908628' |'M348934600' |\n|'C1999090638'|'M348934600' |91       |20.77824175824176 |PatId2   |CHILD     |'C1999090638'|'M348934600' |\n|'C1214229415'|'M1823072687'|136      |22.573161764705883|PatId2   |CHILD     |'C1214229415'|'M1823072687'|\n|'C747353905' |'M1823072687'|89       |22.66876404494382 |PatId2   |CHILD     |'C747353905' |'M1823072687'|\n|'C474891377' |'M1823072687'|120      |22.757583333333333|PatId2   |CHILD     |'C474891377' |'M1823072687'|\n|'C1098443227'|'M1823072687'|80       |22.187625         |PatId2   |CHILD     |'C1098443227'|'M1823072687'|\n|'C1273110804'|'M1823072687'|86       |22.590232558139533|PatId2   |CHILD     |'C1273110804'|'M1823072687'|\n+-------------+-------------+---------+------------------+---------+----------+-------------+-------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    " from pyspark.sql.functions import col, count, avg, lit, countDistinct\n",
    "\n",
    "# Step 1: Group by customer and merchant\n",
    "cust_merchant_stats = df_txn.groupBy(\"customer\", \"merchant\").agg(\n",
    "    count(\"*\").alias(\"txn_count\"),\n",
    "    avg(\"amount\").alias(\"avg_amount\")\n",
    ")\n",
    "\n",
    "# Step 2: See distribution (optional)\n",
    "cust_merchant_stats.describe([\"txn_count\", \"avg_amount\"]).show()\n",
    "\n",
    "# Step 3: Apply filter for PatId2 logic\n",
    "pat2 = cust_merchant_stats.filter(\n",
    "    (col(\"txn_count\") >= 80) & (col(\"avg_amount\") < 23)\n",
    ").withColumn(\"patternId\", lit(\"PatId2\")) \\\n",
    " .withColumn(\"actionType\", lit(\"CHILD\")) \\\n",
    " .withColumn(\"customerName\", col(\"customer\")) \\\n",
    " .withColumn(\"merchantId\", col(\"merchant\"))\n",
    "\n",
    "# Step 4: Show results\n",
    "print(\"PatId2 count:\", pat2.count())\n",
    "pat2.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0014d7e0-90e3-4d5f-81de-d72fb8950b15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Clean gender column (strip single quotes)\n",
    "df_txn_clean = df_txn.withColumn(\"gender\", regexp_replace(\"gender\", \"'\", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c608b2b2-5fc5-48d6-843b-476c9d088098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----+\n|merchant     |F   |M   |\n+-------------+----+----+\n|'M348934600' |2147|1771|\n|'M1823072687'|1945|1617|\n|'M85975013'  |1870|1561|\n|'M855959430' |1594|1329|\n|'M1053599405'|1571|1248|\n|'M151143676' |1497|1226|\n|'M1946091778'|1426|1241|\n|'M209847108' |1183|974 |\n|'M1600850729'|1079|878 |\n|'M1913465890'|1062|844 |\n|'M349281107' |1040|840 |\n|'M480139044' |766 |556 |\n|'M840466850' |618 |507 |\n|'M1535107174'|598 |433 |\n|'M1198415165'|580 |429 |\n|'M78078399'  |552 |440 |\n|'M1649169323'|545 |406 |\n|'M980657600' |458 |336 |\n|'M1842530320'|396 |330 |\n|'M1400236507'|386 |293 |\n+-------------+----+----+\nonly showing top 20 rows\n\nPatId3 Count: 1\n+------------+---+---+---------+----------+------------+------------+\n|merchant    |F  |M  |patternId|actionType|customerName|merchantId  |\n+------------+---+---+---------+----------+------------+------------+\n|'M677738360'|173|174|PatId3   |DEI-NEEDED|            |'M677738360'|\n+------------+---+---+---------+----------+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Use cleaned version here:\n",
    "df_gender_base = df_txn_clean.select(\"merchant\", \"customer\", \"gender\").dropDuplicates()\n",
    "\n",
    "gender_counts = df_gender_base.groupBy(\"merchant\", \"gender\") \\\n",
    "    .agg(countDistinct(\"customer\").alias(\"cust_count\"))\n",
    "\n",
    "gender_pivot = gender_counts.groupBy(\"merchant\") \\\n",
    "    .pivot(\"gender\", [\"F\", \"M\"]).sum(\"cust_count\").na.fill(0)\n",
    "\n",
    "gender_pivot = gender_pivot.select(\n",
    "    col(\"merchant\"),\n",
    "    col(\"F\").cast(\"int\").alias(\"F\"),\n",
    "    col(\"M\").cast(\"int\").alias(\"M\")\n",
    ")\n",
    "\n",
    "gender_pivot.orderBy(col(\"F\").desc()).show(20, truncate=False)\n",
    "\n",
    "pat3 = gender_pivot.filter(\n",
    "    (col(\"F\") > 100) & (col(\"F\") < col(\"M\"))\n",
    ").withColumn(\"patternId\", lit(\"PatId3\")) \\\n",
    " .withColumn(\"actionType\", lit(\"DEI-NEEDED\")) \\\n",
    " .withColumn(\"customerName\", lit(\"\")) \\\n",
    " .withColumn(\"merchantId\", col(\"merchant\"))\n",
    "\n",
    "print(\"PatId3 Count:\", pat3.count())\n",
    "pat3.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5689898d-9906-4739-9252-3013ca72ddda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Total rows: 594643, Chunks: 60\n✅ Wrote chunk 0 (0–10000)\n⏭️ Chunk 1 already exists. Skipping...\n⏭️ Chunk 2 already exists. Skipping...\n⏭️ Chunk 3 already exists. Skipping...\n⏭️ Chunk 4 already exists. Skipping...\n✅ Wrote chunk 5 (50000–60000)\n⏭️ Chunk 6 already exists. Skipping...\n⏭️ Chunk 7 already exists. Skipping...\n✅ Wrote chunk 8 (80000–90000)\n✅ Wrote chunk 9 (90000–100000)\n✅ Wrote chunk 10 (100000–110000)\n✅ Wrote chunk 11 (110000–120000)\n⏭️ Chunk 12 already exists. Skipping...\n⏭️ Chunk 13 already exists. Skipping...\n✅ Wrote chunk 14 (140000–150000)\n⏭️ Chunk 15 already exists. Skipping...\n⏭️ Chunk 16 already exists. Skipping...\n⏭️ Chunk 17 already exists. Skipping...\n✅ Wrote chunk 18 (180000–190000)\n⏭️ Chunk 19 already exists. Skipping...\n⏭️ Chunk 20 already exists. Skipping...\n⏭️ Chunk 21 already exists. Skipping...\n✅ Wrote chunk 22 (220000–230000)\n✅ Wrote chunk 23 (230000–240000)\n✅ Wrote chunk 24 (240000–250000)\n⏭️ Chunk 25 already exists. Skipping...\n✅ Wrote chunk 26 (260000–270000)\n⏭️ Chunk 27 already exists. Skipping...\n✅ Wrote chunk 28 (280000–290000)\n✅ Wrote chunk 29 (290000–300000)\n✅ Wrote chunk 30 (300000–310000)\n⏭️ Chunk 31 already exists. Skipping...\n⏭️ Chunk 32 already exists. Skipping...\n⏭️ Chunk 33 already exists. Skipping...\n✅ Wrote chunk 34 (340000–350000)\n⏭️ Chunk 35 already exists. Skipping...\n⏭️ Chunk 36 already exists. Skipping...\n⏭️ Chunk 37 already exists. Skipping...\n✅ Wrote chunk 38 (380000–390000)\n⏭️ Chunk 39 already exists. Skipping...\n⏭️ Chunk 40 already exists. Skipping...\n⏭️ Chunk 41 already exists. Skipping...\n✅ Wrote chunk 42 (420000–430000)\n⏭️ Chunk 43 already exists. Skipping...\n✅ Wrote chunk 44 (440000–450000)\n✅ Wrote chunk 45 (450000–460000)\n✅ Wrote chunk 46 (460000–470000)\n✅ Wrote chunk 47 (470000–480000)\n✅ Wrote chunk 48 (480000–490000)\n✅ Wrote chunk 49 (490000–500000)\n⏭️ Chunk 50 already exists. Skipping...\n⏭️ Chunk 51 already exists. Skipping...\n✅ Wrote chunk 52 (520000–530000)\n⏭️ Chunk 53 already exists. Skipping...\n✅ Wrote chunk 54 (540000–550000)\n⏭️ Chunk 55 already exists. Skipping...\n✅ Wrote chunk 56 (560000–570000)\n⏭️ Chunk 57 already exists. Skipping...\n✅ Wrote chunk 58 (580000–590000)\n✅ Wrote chunk 59 (590000–600000)\n🎉 Stream simulation complete.\n"
     ]
    }
   ],
   "source": [
    "# Mechanism X - Enhanced with PostgreSQL Tracking\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "import psycopg2\n",
    "\n",
    "# Configuration\n",
    "chunk_size = 10000\n",
    "bucket_name = \"bucket-aswanthlal-20250609123312\"\n",
    "output_dir = f\"s3a://{bucket_name}/stream_chunks\"\n",
    "access_key = \"AKIAU76LUFEGVNYPMUVE\"\n",
    "secret_key = \"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\"\n",
    "pg_conn_params = {\n",
    "    'host': 'txndb.cvskc4cqgii5.ap-south-1.rds.amazonaws.com',\n",
    "    'port': 5432,\n",
    "    'dbname': 'postgres',\n",
    "    'user': 'txn_postgres',\n",
    "    'password': 'rootroot'\n",
    "}\n",
    "\n",
    "# Step 1: Setup S3 credentials\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.ap-south-1.amazonaws.com\")\n",
    "\n",
    "# Step 2: Add row index to DataFrame\n",
    "df_txn_indexed = df_txn.withColumn(\n",
    "    \"row_id\", row_number().over(Window.orderBy(\"customer\", \"merchant\", \"amount\"))\n",
    ")\n",
    "total_rows = df_txn_indexed.count()\n",
    "num_chunks = (total_rows + chunk_size - 1) // chunk_size\n",
    "print(f\"🔢 Total rows: {total_rows}, Chunks: {num_chunks}\")\n",
    "\n",
    "# Step 3: PostgreSQL setup\n",
    "create_table_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS processed_chunks (\n",
    "    chunk_index INT PRIMARY KEY,\n",
    "    chunk_name TEXT NOT NULL,\n",
    "    s3_path TEXT NOT NULL,\n",
    "    processed_at TIMESTAMP DEFAULT now()\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "insert_chunk_sql = \"\"\"\n",
    "INSERT INTO processed_chunks (chunk_index, chunk_name, s3_path)\n",
    "VALUES (%s, %s, %s)\n",
    "ON CONFLICT (chunk_index) DO NOTHING;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "select_chunks_sql = \"SELECT chunk_index FROM processed_chunks;\"\n",
    "\n",
    "# Step 4: Connect to PostgreSQL and get existing chunks\n",
    "conn = psycopg2.connect(**pg_conn_params)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(create_table_sql)\n",
    "conn.commit()\n",
    "cursor.execute(select_chunks_sql)\n",
    "existing_indices = {row[0] for row in cursor.fetchall()}\n",
    "\n",
    "# Step 5: Loop through chunks\n",
    "for chunk_index in range(num_chunks):\n",
    "    if chunk_index in existing_indices:\n",
    "        print(f\"⏭️ Chunk {chunk_index} already exists. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    start_idx = chunk_index * chunk_size\n",
    "    end_idx = start_idx + chunk_size\n",
    "\n",
    "    chunk_df = df_txn_indexed.filter(\n",
    "        (col(\"row_id\") > start_idx) & (col(\"row_id\") <= end_idx)\n",
    "    ).drop(\"row_id\")\n",
    "\n",
    "    if chunk_df.rdd.isEmpty():\n",
    "        print(f\"⚠️ Empty chunk at index {chunk_index}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    timestamp = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    chunk_name = f\"transactions_chunk_{chunk_index}_{timestamp}.csv\"\n",
    "    chunk_path = f\"{output_dir}/transactions_chunk_{chunk_index}_{timestamp}.csv\"\n",
    "    chunk_df.write.mode(\"overwrite\").option(\"header\", True).csv(chunk_path)\n",
    "    print(f\"✅ Wrote chunk {chunk_index} ({start_idx}–{end_idx})\")\n",
    "\n",
    "    # Log to PostgreSQL\n",
    "    cursor.execute(insert_chunk_sql, (chunk_index, chunk_name, chunk_path))\n",
    "    conn.commit()\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"🎉 Stream simulation complete.\")\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e9cbe96-1c55-45d9-b0cb-ede63b9ea213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_10_20250618061430.csv/</td><td>transactions_chunk_10_20250618061430.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_11_20250618061449.csv/</td><td>transactions_chunk_11_20250618061449.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_12_20250618061509.csv/</td><td>transactions_chunk_12_20250618061509.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_13_20250618061527.csv/</td><td>transactions_chunk_13_20250618061527.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_14_20250618061547.csv/</td><td>transactions_chunk_14_20250618061547.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_15_20250618061605.csv/</td><td>transactions_chunk_15_20250618061605.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_16_20250618061624.csv/</td><td>transactions_chunk_16_20250618061624.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_17_20250618061643.csv/</td><td>transactions_chunk_17_20250618061643.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_18_20250618061701.csv/</td><td>transactions_chunk_18_20250618061701.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_19_20250618061720.csv/</td><td>transactions_chunk_19_20250618061720.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_1_20250618061139.csv/</td><td>transactions_chunk_1_20250618061139.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_20_20250618061738.csv/</td><td>transactions_chunk_20_20250618061738.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_21_20250618061757.csv/</td><td>transactions_chunk_21_20250618061757.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_22_20250618061816.csv/</td><td>transactions_chunk_22_20250618061816.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_23_20250618061834.csv/</td><td>transactions_chunk_23_20250618061834.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_24_20250618061852.csv/</td><td>transactions_chunk_24_20250618061852.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_25_20250618061911.csv/</td><td>transactions_chunk_25_20250618061911.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_26_20250618061929.csv/</td><td>transactions_chunk_26_20250618061929.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_27_20250618061948.csv/</td><td>transactions_chunk_27_20250618061948.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_28_20250618062007.csv/</td><td>transactions_chunk_28_20250618062007.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_29_20250618062029.csv/</td><td>transactions_chunk_29_20250618062029.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_2_20250618061159.csv/</td><td>transactions_chunk_2_20250618061159.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_30_20250618062048.csv/</td><td>transactions_chunk_30_20250618062048.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_31_20250618062107.csv/</td><td>transactions_chunk_31_20250618062107.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_32_20250618062125.csv/</td><td>transactions_chunk_32_20250618062125.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_33_20250618062143.csv/</td><td>transactions_chunk_33_20250618062143.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_34_20250618062201.csv/</td><td>transactions_chunk_34_20250618062201.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_35_20250618062219.csv/</td><td>transactions_chunk_35_20250618062219.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_36_20250618062237.csv/</td><td>transactions_chunk_36_20250618062237.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_37_20250618062255.csv/</td><td>transactions_chunk_37_20250618062255.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_38_20250618062313.csv/</td><td>transactions_chunk_38_20250618062313.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_39_20250618062331.csv/</td><td>transactions_chunk_39_20250618062331.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_3_20250618061218.csv/</td><td>transactions_chunk_3_20250618061218.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_40_20250618062349.csv/</td><td>transactions_chunk_40_20250618062349.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_41_20250618062408.csv/</td><td>transactions_chunk_41_20250618062408.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_42_20250618062426.csv/</td><td>transactions_chunk_42_20250618062426.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_43_20250618062443.csv/</td><td>transactions_chunk_43_20250618062443.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_44_20250618062501.csv/</td><td>transactions_chunk_44_20250618062501.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_45_20250618062519.csv/</td><td>transactions_chunk_45_20250618062519.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_46_20250618062537.csv/</td><td>transactions_chunk_46_20250618062537.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_47_20250618062556.csv/</td><td>transactions_chunk_47_20250618062556.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_48_20250618062615.csv/</td><td>transactions_chunk_48_20250618062615.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_49_20250618062633.csv/</td><td>transactions_chunk_49_20250618062633.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_4_20250618061237.csv/</td><td>transactions_chunk_4_20250618061237.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_50_20250618062651.csv/</td><td>transactions_chunk_50_20250618062651.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_51_20250618062709.csv/</td><td>transactions_chunk_51_20250618062709.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_52_20250618062728.csv/</td><td>transactions_chunk_52_20250618062728.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_53_20250618062746.csv/</td><td>transactions_chunk_53_20250618062746.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_54_20250618062804.csv/</td><td>transactions_chunk_54_20250618062804.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_55_20250618062822.csv/</td><td>transactions_chunk_55_20250618062822.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_56_20250618062841.csv/</td><td>transactions_chunk_56_20250618062841.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_57_20250618062900.csv/</td><td>transactions_chunk_57_20250618062900.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_58_20250618062918.csv/</td><td>transactions_chunk_58_20250618062918.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_59_20250618062937.csv/</td><td>transactions_chunk_59_20250618062937.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_5_20250618061256.csv/</td><td>transactions_chunk_5_20250618061256.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_6_20250618061315.csv/</td><td>transactions_chunk_6_20250618061315.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_7_20250618061334.csv/</td><td>transactions_chunk_7_20250618061334.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_8_20250618061353.csv/</td><td>transactions_chunk_8_20250618061353.csv/</td><td>0</td><td>1750228267000</td></tr><tr><td>s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_9_20250618061411.csv/</td><td>transactions_chunk_9_20250618061411.csv/</td><td>0</td><td>1750228267000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_10_20250618061430.csv/",
         "transactions_chunk_10_20250618061430.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_11_20250618061449.csv/",
         "transactions_chunk_11_20250618061449.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_12_20250618061509.csv/",
         "transactions_chunk_12_20250618061509.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_13_20250618061527.csv/",
         "transactions_chunk_13_20250618061527.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_14_20250618061547.csv/",
         "transactions_chunk_14_20250618061547.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_15_20250618061605.csv/",
         "transactions_chunk_15_20250618061605.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_16_20250618061624.csv/",
         "transactions_chunk_16_20250618061624.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_17_20250618061643.csv/",
         "transactions_chunk_17_20250618061643.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_18_20250618061701.csv/",
         "transactions_chunk_18_20250618061701.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_19_20250618061720.csv/",
         "transactions_chunk_19_20250618061720.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_1_20250618061139.csv/",
         "transactions_chunk_1_20250618061139.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_20_20250618061738.csv/",
         "transactions_chunk_20_20250618061738.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_21_20250618061757.csv/",
         "transactions_chunk_21_20250618061757.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_22_20250618061816.csv/",
         "transactions_chunk_22_20250618061816.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_23_20250618061834.csv/",
         "transactions_chunk_23_20250618061834.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_24_20250618061852.csv/",
         "transactions_chunk_24_20250618061852.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_25_20250618061911.csv/",
         "transactions_chunk_25_20250618061911.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_26_20250618061929.csv/",
         "transactions_chunk_26_20250618061929.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_27_20250618061948.csv/",
         "transactions_chunk_27_20250618061948.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_28_20250618062007.csv/",
         "transactions_chunk_28_20250618062007.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_29_20250618062029.csv/",
         "transactions_chunk_29_20250618062029.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_2_20250618061159.csv/",
         "transactions_chunk_2_20250618061159.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_30_20250618062048.csv/",
         "transactions_chunk_30_20250618062048.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_31_20250618062107.csv/",
         "transactions_chunk_31_20250618062107.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_32_20250618062125.csv/",
         "transactions_chunk_32_20250618062125.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_33_20250618062143.csv/",
         "transactions_chunk_33_20250618062143.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_34_20250618062201.csv/",
         "transactions_chunk_34_20250618062201.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_35_20250618062219.csv/",
         "transactions_chunk_35_20250618062219.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_36_20250618062237.csv/",
         "transactions_chunk_36_20250618062237.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_37_20250618062255.csv/",
         "transactions_chunk_37_20250618062255.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_38_20250618062313.csv/",
         "transactions_chunk_38_20250618062313.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_39_20250618062331.csv/",
         "transactions_chunk_39_20250618062331.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_3_20250618061218.csv/",
         "transactions_chunk_3_20250618061218.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_40_20250618062349.csv/",
         "transactions_chunk_40_20250618062349.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_41_20250618062408.csv/",
         "transactions_chunk_41_20250618062408.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_42_20250618062426.csv/",
         "transactions_chunk_42_20250618062426.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_43_20250618062443.csv/",
         "transactions_chunk_43_20250618062443.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_44_20250618062501.csv/",
         "transactions_chunk_44_20250618062501.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_45_20250618062519.csv/",
         "transactions_chunk_45_20250618062519.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_46_20250618062537.csv/",
         "transactions_chunk_46_20250618062537.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_47_20250618062556.csv/",
         "transactions_chunk_47_20250618062556.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_48_20250618062615.csv/",
         "transactions_chunk_48_20250618062615.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_49_20250618062633.csv/",
         "transactions_chunk_49_20250618062633.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_4_20250618061237.csv/",
         "transactions_chunk_4_20250618061237.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_50_20250618062651.csv/",
         "transactions_chunk_50_20250618062651.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_51_20250618062709.csv/",
         "transactions_chunk_51_20250618062709.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_52_20250618062728.csv/",
         "transactions_chunk_52_20250618062728.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_53_20250618062746.csv/",
         "transactions_chunk_53_20250618062746.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_54_20250618062804.csv/",
         "transactions_chunk_54_20250618062804.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_55_20250618062822.csv/",
         "transactions_chunk_55_20250618062822.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_56_20250618062841.csv/",
         "transactions_chunk_56_20250618062841.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_57_20250618062900.csv/",
         "transactions_chunk_57_20250618062900.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_58_20250618062918.csv/",
         "transactions_chunk_58_20250618062918.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_59_20250618062937.csv/",
         "transactions_chunk_59_20250618062937.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_5_20250618061256.csv/",
         "transactions_chunk_5_20250618061256.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_6_20250618061315.csv/",
         "transactions_chunk_6_20250618061315.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_7_20250618061334.csv/",
         "transactions_chunk_7_20250618061334.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_8_20250618061353.csv/",
         "transactions_chunk_8_20250618061353.csv/",
         0,
         1750228267000
        ],
        [
         "s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_9_20250618061411.csv/",
         "transactions_chunk_9_20250618061411.csv/",
         0,
         1750228267000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = \"s3a://bucket-aswanthlal-20250609123312/stream_chunks\"\n",
    "files = dbutils.fs.ls(output_dir)\n",
    "display(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1595f769-bc82-4944-b1ac-6c8298eb50dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️ Skipping already processed chunk: transactions_chunk_0_20250618083231.csv\n🔄 Processing: transactions_chunk_10_20250618061430.csv\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4062083012085494>:142\u001B[0m\n",
       "\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunk_index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    141\u001B[0m     processed_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame([(chunk_index, chunk_name, file_path)], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchunk_index\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchunk_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ms3_path\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
       "\u001B[0;32m--> 142\u001B[0m     processed_df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mjdbc(url\u001B[38;5;241m=\u001B[39mjdbc_url, table\u001B[38;5;241m=\u001B[39mprocessed_table, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m, properties\u001B[38;5;241m=\u001B[39mjdbc_props)\n",
       "\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    144\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m⚠️ Unable to extract chunk_index from chunk_name: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mchunk_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Skipping insert.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1915\u001B[0m, in \u001B[0;36mDataFrameWriter.jdbc\u001B[0;34m(self, url, table, mode, properties)\u001B[0m\n",
       "\u001B[1;32m   1913\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m properties:\n",
       "\u001B[1;32m   1914\u001B[0m     jprop\u001B[38;5;241m.\u001B[39msetProperty(k, properties[k])\n",
       "\u001B[0;32m-> 1915\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjdbc\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjprop\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o50890.jdbc.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 4027.0 failed 1 times, most recent failure: Lost task 7.0 in stage 4027.0 (TID 4347) (ip-10-172-161-90.us-west-2.compute.internal executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO processed_chunks (\"chunk_index\",\"chunk_name\",\"s3_path\") VALUES (10,'transactions_chunk_10_20250618061430.csv','s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_10_20250618061430.csv/') was aborted: ERROR: duplicate key value violates unique constraint \"processed_chunks_pkey\"\n",
       "  Detail: Key (chunk_index)=(10) already exists.  Call getNextException to see other errors in the batch.\n",
       "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
       "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
       "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
       "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:559)\n",
       "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:887)\n",
       "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:910)\n",
       "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1638)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:899)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:898)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1059)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1059)\n",
       "\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2798)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"processed_chunks_pkey\"\n",
       "  Detail: Key (chunk_index)=(10) already exists.\n",
       "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2675)\n",
       "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2365)\n",
       "\t... 30 more\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3440)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3362)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3351)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3351)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1460)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1460)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1460)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3651)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3589)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3577)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1209)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1197)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2741)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2779)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2798)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2823)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1059)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n",
       "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1057)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:898)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.saveTableToJDBC(JdbcRelationProvider.scala:72)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:96)\n",
       "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:49)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:82)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:79)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:91)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:305)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:429)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:258)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:861)\n",
       "\tat sun.reflect.GeneratedMethodAccessor1102.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO processed_chunks (\"chunk_index\",\"chunk_name\",\"s3_path\") VALUES (10,'transactions_chunk_10_20250618061430.csv','s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_10_20250618061430.csv/') was aborted: ERROR: duplicate key value violates unique constraint \"processed_chunks_pkey\"\n",
       "  Detail: Key (chunk_index)=(10) already exists.  Call getNextException to see other errors in the batch.\n",
       "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
       "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
       "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
       "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:559)\n",
       "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:887)\n",
       "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:910)\n",
       "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1638)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:899)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:898)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1059)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1059)\n",
       "\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2798)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n",
       "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"processed_chunks_pkey\"\n",
       "  Detail: Key (chunk_index)=(10) already exists.\n",
       "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2675)\n",
       "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2365)\n",
       "\t... 30 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-4062083012085494>:142\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunk_index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    141\u001B[0m     processed_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame([(chunk_index, chunk_name, file_path)], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchunk_index\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchunk_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ms3_path\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m--> 142\u001B[0m     processed_df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mjdbc(url\u001B[38;5;241m=\u001B[39mjdbc_url, table\u001B[38;5;241m=\u001B[39mprocessed_table, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m, properties\u001B[38;5;241m=\u001B[39mjdbc_props)\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m⚠️ Unable to extract chunk_index from chunk_name: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mchunk_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Skipping insert.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1915\u001B[0m, in \u001B[0;36mDataFrameWriter.jdbc\u001B[0;34m(self, url, table, mode, properties)\u001B[0m\n\u001B[1;32m   1913\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m properties:\n\u001B[1;32m   1914\u001B[0m     jprop\u001B[38;5;241m.\u001B[39msetProperty(k, properties[k])\n\u001B[0;32m-> 1915\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjdbc\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjprop\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o50890.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 4027.0 failed 1 times, most recent failure: Lost task 7.0 in stage 4027.0 (TID 4347) (ip-10-172-161-90.us-west-2.compute.internal executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO processed_chunks (\"chunk_index\",\"chunk_name\",\"s3_path\") VALUES (10,'transactions_chunk_10_20250618061430.csv','s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_10_20250618061430.csv/') was aborted: ERROR: duplicate key value violates unique constraint \"processed_chunks_pkey\"\n  Detail: Key (chunk_index)=(10) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:559)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:887)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:910)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1638)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:899)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1059)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1059)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2798)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"processed_chunks_pkey\"\n  Detail: Key (chunk_index)=(10) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2675)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2365)\n\t... 30 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3440)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3362)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1460)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1460)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3651)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3589)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3577)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1209)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1197)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2741)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2779)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2798)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2823)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1059)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1057)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:898)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.saveTableToJDBC(JdbcRelationProvider.scala:72)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:96)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:49)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:79)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:305)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:429)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:258)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:861)\n\tat sun.reflect.GeneratedMethodAccessor1102.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO processed_chunks (\"chunk_index\",\"chunk_name\",\"s3_path\") VALUES (10,'transactions_chunk_10_20250618061430.csv','s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_10_20250618061430.csv/') was aborted: ERROR: duplicate key value violates unique constraint \"processed_chunks_pkey\"\n  Detail: Key (chunk_index)=(10) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:559)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:887)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:910)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1638)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:899)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1059)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1059)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2798)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"processed_chunks_pkey\"\n  Detail: Key (chunk_index)=(10) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2675)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2365)\n\t... 30 more\n",
       "errorSummary": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 4027.0 failed 1 times, most recent failure: Lost task 7.0 in stage 4027.0 (TID 4347) (ip-10-172-161-90.us-west-2.compute.internal executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO processed_chunks (\"chunk_index\",\"chunk_name\",\"s3_path\") VALUES (10,'transactions_chunk_10_20250618061430.csv','s3a://bucket-aswanthlal-20250609123312/stream_chunks/transactions_chunk_10_20250618061430.csv/') was aborted: ERROR: duplicate key value violates unique constraint \"processed_chunks_pkey\"",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, coalesce, current_timestamp, date_format, from_utc_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "import psycopg2\n",
    "\n",
    "# Configuration\n",
    "bucket = \"bucket-aswanthlal-20250609123312\"\n",
    "chunk_dir = f\"s3a://{bucket}/stream_chunks\"\n",
    "output_dir = f\"s3a://{bucket}/pattern_detections\"\n",
    "jdbc_url = \"jdbc:postgresql://txndb.cvskc4cqgii5.ap-south-1.rds.amazonaws.com:5432/postgres\"\n",
    "jdbc_props = {\n",
    "    \"user\": \"txn_postgres\",\n",
    "    \"password\": \"rootroot\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "processed_table = \"processed_chunks\"\n",
    "\n",
    "# Schema\n",
    "chunk_schema = StructType([\n",
    "    StructField(\"step\", IntegerType(), True),\n",
    "    StructField(\"customer\", StringType(), True),\n",
    "    StructField(\"age\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"zipcodeOri\", StringType(), True),\n",
    "    StructField(\"merchant\", StringType(), True),\n",
    "    StructField(\"zipMerchant\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"fraud\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MechanismY\").getOrCreate()\n",
    "\n",
    "def get_processed_chunks():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"txndb.cvskc4cqgii5.ap-south-1.rds.amazonaws.com\",\n",
    "        dbname=\"postgres\",\n",
    "        user=\"txn_postgres\",\n",
    "        password=\"rootroot\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT chunk_name FROM processed_chunks\")\n",
    "    rows = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return {row[0] for row in rows}\n",
    "\n",
    "df_importance = spark.read.csv(\"dbfs:/tmp/transactions/CustomerImportance.csv\", header=True, inferSchema=True)\n",
    "df_importance = df_importance.withColumnRenamed(\"Source\", \"customer\") \\\n",
    "                             .withColumnRenamed(\"Target\", \"merchant\") \\\n",
    "                             .withColumnRenamed(\"typeTrans\", \"category\")\n",
    "\n",
    "def detect_patid1(df_txn: DataFrame, df_imp: DataFrame) -> DataFrame:\n",
    "    df_joined = df_txn.join(df_imp, [\"customer\", \"merchant\"], how=\"inner\")\n",
    "    merchant_txn_counts = df_txn.groupBy(\"merchant\").count().withColumnRenamed(\"count\", \"total_txns\")\n",
    "    eligible_merchants = merchant_txn_counts.filter(F.col(\"total_txns\") > 50000)\n",
    "    df_joined = df_joined.join(eligible_merchants, \"merchant\", how=\"inner\")\n",
    "\n",
    "    agg_df = df_joined.groupBy(\"customer\", \"merchant\").agg(\n",
    "        F.count(\"*\").alias(\"txn_count\"),\n",
    "        F.avg(\"Weight\").alias(\"avg_weight\")\n",
    "    )\n",
    "\n",
    "    txn_window = Window.partitionBy(\"merchant\").orderBy(F.desc(\"txn_count\"))\n",
    "    weight_window = Window.partitionBy(\"merchant\").orderBy(\"avg_weight\")\n",
    "\n",
    "    return agg_df.withColumn(\"txn_rank\", F.percent_rank().over(txn_window)) \\\n",
    "                 .withColumn(\"weight_rank\", F.percent_rank().over(weight_window)) \\\n",
    "                 .filter((F.col(\"txn_rank\") <= 0.01) & (F.col(\"weight_rank\") <= 0.01)) \\\n",
    "                 .withColumn(\"patternId\", lit(\"PatId1\")) \\\n",
    "                 .withColumn(\"actionType\", lit(\"UPGRADE\"))\n",
    "\n",
    "def detect_patid2(df_txn: DataFrame) -> DataFrame:\n",
    "    return df_txn.groupBy(\"customer\", \"merchant\").agg(\n",
    "        F.count(\"*\").alias(\"txn_count\"),\n",
    "        F.avg(\"amount\").alias(\"avg_amount\")\n",
    "    ).filter((F.col(\"txn_count\") >= 80) & (F.col(\"avg_amount\") < 23)) \\\n",
    "     .withColumn(\"patternId\", lit(\"PatId2\")) \\\n",
    "     .withColumn(\"actionType\", lit(\"CHILD\"))\n",
    "\n",
    "def detect_patid3(df_txn: DataFrame) -> DataFrame:\n",
    "    gender_counts = df_txn.select(\"merchant\", \"customer\", \"gender\").dropDuplicates() \\\n",
    "        .groupBy(\"merchant\", \"gender\").agg(F.countDistinct(\"customer\").alias(\"cust_count\")) \\\n",
    "        .groupBy(\"merchant\").pivot(\"gender\", [\"F\", \"M\"]).sum(\"cust_count\").na.fill(0)\n",
    "\n",
    "    return gender_counts.filter((F.col(\"F\") > 100) & (F.col(\"F\") < F.col(\"M\"))) \\\n",
    "        .withColumn(\"patternId\", lit(\"PatId3\")) \\\n",
    "        .withColumn(\"actionType\", lit(\"DEI-NEEDED\")) \\\n",
    "        .withColumn(\"customer\", lit(\"\"))\n",
    "\n",
    "processed_chunks = get_processed_chunks()\n",
    "all_files = [f.path for f in dbutils.fs.ls(chunk_dir) if f.path.endswith(\".csv/\")]\n",
    "\n",
    "for file_path in all_files:\n",
    "    chunk_name = os.path.basename(file_path.rstrip(\"/\"))\n",
    "    if chunk_name in processed_chunks:\n",
    "        print(f\"⏭️ Skipping already processed chunk: {chunk_name}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"🔄 Processing: {chunk_name}\")\n",
    "    df_chunk = spark.read.schema(chunk_schema).option(\"header\", True).csv(file_path)\n",
    "\n",
    "    # Run pattern detections\n",
    "    patid1_df = detect_patid1(df_chunk, df_importance).select(\"customer\", \"merchant\", \"patternId\", \"actionType\")\n",
    "    patid2_df = detect_patid2(df_chunk).select(\"customer\", \"merchant\", \"patternId\", \"actionType\")\n",
    "    patid3_df = detect_patid3(df_chunk).select(\"customer\", \"merchant\", \"patternId\", \"actionType\")\n",
    "\n",
    "    detections = patid1_df.unionByName(patid2_df).unionByName(patid3_df)\n",
    "\n",
    "    df_final = df_chunk.join(detections, on=[\"customer\", \"merchant\"], how=\"left\")\n",
    "\n",
    "    y_start_time = datetime.now(timezone(\"Asia/Kolkata\")).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    df_final = df_final.withColumn(\"YStartTime\", lit(y_start_time)) \\\n",
    "                       .withColumn(\"detectionTime\", current_timestamp()) \\\n",
    "                       .withColumn(\"detectionTime\", date_format(from_utc_timestamp(\"detectionTime\", \"Asia/Kolkata\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                       .withColumn(\"patternId\", coalesce(F.col(\"patternId\"), lit(\"\"))) \\\n",
    "                       .withColumn(\"actionType\", coalesce(F.col(\"actionType\"), lit(\"\"))) \\\n",
    "                       .withColumn(\"customerName\", coalesce(F.col(\"customer\"), lit(\"\"))) \\\n",
    "                       .withColumn(\"merchantId\", coalesce(F.col(\"merchant\"), lit(\"\")))\n",
    "\n",
    "    df_final = df_final.select(\"YStartTime\", \"detectionTime\", \"patternId\", \"actionType\", \"customerName\", \"merchantId\")\n",
    "\n",
    "    rows = df_final.collect()\n",
    "    for i in range(0, len(rows), 50):\n",
    "        batch_df = spark.createDataFrame(rows[i:i+50], df_final.schema)\n",
    "        unique_id = uuid.uuid4().hex\n",
    "        out_path = f\"{output_dir}/pattern_output_{chunk_name}_batch{i//50 + 1}_{unique_id}\"\n",
    "        batch_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(out_path)\n",
    "\n",
    "    try:\n",
    "        chunk_index = int(chunk_name.split(\"_\")[2])\n",
    "    except (IndexError, ValueError):\n",
    "        chunk_index = None\n",
    "\n",
    "    if chunk_index is not None:\n",
    "        processed_df = spark.createDataFrame([(chunk_index, chunk_name, file_path)], [\"chunk_index\", \"chunk_name\", \"s3_path\"])\n",
    "        processed_df.write.jdbc(url=jdbc_url, table=processed_table, mode=\"append\", properties=jdbc_props)\n",
    "    else:\n",
    "        print(f\"⚠️ Unable to extract chunk_index from chunk_name: {chunk_name}. Skipping insert.\")\n",
    "\n",
    "    print(f\"✅ Finished: {chunk_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d70a65c0-923f-4005-ae5c-477d794616c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Zipped and uploaded to S3:\ns3a://bucket-aswanthlal-20250609123312/final_outputs/full_output_20250612113442.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Config\n",
    "bucket = \"bucket-aswanthlal-20250609123312\"\n",
    "chunk_dir = f\"s3a://{bucket}/stream_chunks\"\n",
    "pattern_dir = f\"s3a://{bucket}/pattern_detections\"\n",
    "local_base_dir = \"/dbfs/tmp/export_all\"\n",
    "local_chunk_dir = f\"{local_base_dir}/chunks\"\n",
    "local_pattern_dir = f\"{local_base_dir}/patterns\"\n",
    "zip_filename = f\"full_output_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}.zip\"\n",
    "local_zip_path = f\"/dbfs/tmp/{zip_filename}\"\n",
    "s3_zip_path = f\"s3a://{bucket}/final_outputs/{zip_filename}\"\n",
    "\n",
    "# Prepare directories\n",
    "# Prepare DBFS + local directories\n",
    "dbutils.fs.mkdirs(\"dbfs:/tmp/export_all/chunks\")\n",
    "dbutils.fs.mkdirs(\"dbfs:/tmp/export_all/patterns\")\n",
    "\n",
    "os.makedirs(\"/dbfs/tmp/export_all/chunks\", exist_ok=True)\n",
    "os.makedirs(\"/dbfs/tmp/export_all/patterns\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Helper to download actual CSV files (Spark writes CSVs in folder)\n",
    "def download_csvs(s3_folder_path, local_target_dir):\n",
    "    folders = [f.path for f in dbutils.fs.ls(s3_folder_path) if f.path.endswith(\".csv/\")]\n",
    "    for folder in folders:\n",
    "        csv_files = [f.path for f in dbutils.fs.ls(folder) if f.path.endswith(\".csv\")]\n",
    "        for s3_csv in csv_files:\n",
    "            file_name = os.path.basename(s3_csv)\n",
    "            local_path = f\"{local_target_dir}/{file_name}\"\n",
    "            dbutils.fs.cp(s3_csv, f\"file:{local_path}\")\n",
    "\n",
    "# Step 1: Download chunk files\n",
    "download_csvs(chunk_dir, local_chunk_dir)\n",
    "\n",
    "# Step 2: Download pattern detection files\n",
    "download_csvs(pattern_dir, local_pattern_dir)\n",
    "\n",
    "# Step 3: Zip all files\n",
    "with zipfile.ZipFile(local_zip_path, 'w') as zipf:\n",
    "    for folder, label in [(local_chunk_dir, \"chunks\"), (local_pattern_dir, \"patterns\")]:\n",
    "        for file_name in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, file_name)\n",
    "            arcname = f\"{label}/{file_name}\"  # Keep files organized in zip\n",
    "            zipf.write(file_path, arcname=arcname)\n",
    "\n",
    "# Step 4: Upload ZIP to S3\n",
    "dbutils.fs.cp(f\"file:{local_zip_path}\", s3_zip_path)\n",
    "\n",
    "print(f\"✅ Zipped and uploaded to S3:\\n{s3_zip_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mechanism_x_y",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}