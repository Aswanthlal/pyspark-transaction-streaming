{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb230474-f8b7-49b6-bbba-0a0aa6d88b03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: gdown in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c1b2d373-db54-4cf4-9b14-d672c447a605/lib/python3.9/site-packages (5.2.0)\nRequirement already satisfied: requests[socks] in /databricks/python3/lib/python3.9/site-packages (from gdown) (2.27.1)\nRequirement already satisfied: beautifulsoup4 in /databricks/python3/lib/python3.9/site-packages (from gdown) (4.11.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown) (3.9.0)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c1b2d373-db54-4cf4-9b14-d672c447a605/lib/python3.9/site-packages (from gdown) (4.67.1)\nRequirement already satisfied: soupsieve>1.2 in /databricks/python3/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.3.1)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests[socks]->gdown) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests[socks]->gdown) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests[socks]->gdown) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests[socks]->gdown) (1.26.9)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c1b2d373-db54-4cf4-9b14-d672c447a605/lib/python3.9/site-packages (from requests[socks]->gdown) (1.7.1)\nPython interpreter will be restarted.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install gdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f296e7a0-6410-4f94-981c-5c6e5a30d9f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1abe9EkM_uf2F2hjEkbhMBG9Mf2dFE4Wo CustomerImportance.csv\nProcessing file 1AGXVlDhbMbhoGXDJG0IThnqz86Qy3hqb transactions.csv\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom: https://drive.google.com/uc?id=1abe9EkM_uf2F2hjEkbhMBG9Mf2dFE4Wo\nTo: /dbfs/tmp/transactions/CustomerImportance.csv\n\r  0%|          | 0.00/32.7M [00:00<?, ?B/s]\r 21%|██        | 6.82M/32.7M [00:00<00:00, 62.7MB/s]\r 74%|███████▍  | 24.1M/32.7M [00:00<00:00, 123MB/s] \r100%|██████████| 32.7M/32.7M [00:00<00:00, 109MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1AGXVlDhbMbhoGXDJG0IThnqz86Qy3hqb\nTo: /dbfs/tmp/transactions/transactions.csv\n\r  0%|          | 0.00/49.0M [00:00<?, ?B/s]\r 14%|█▍        | 6.82M/49.0M [00:00<00:00, 47.8MB/s]\r 52%|█████▏    | 25.7M/49.0M [00:00<00:00, 115MB/s] \r 78%|███████▊  | 38.3M/49.0M [00:00<00:00, 117MB/s]\r100%|██████████| 49.0M/49.0M [00:00<00:00, 119MB/s]\nDownload completed\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: ['/dbfs/tmp/transactions/CustomerImportance.csv',\n '/dbfs/tmp/transactions/transactions.csv']"
     ]
    }
   ],
   "source": [
    "#importing data from GD\n",
    "import gdown\n",
    "\n",
    "url = \"https://drive.google.com/drive/folders/1qryhdlgNsmecWRy2haI8S3uC63wKk5X-\"\n",
    "output_path = \"/dbfs/tmp/transactions\"  # Where to save it\n",
    "\n",
    "gdown.download_folder(url=url, output=output_path, quiet=False, use_cookies=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c04a8c6-82ec-4af4-bd30-98a9170aa221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: True"
     ]
    }
   ],
   "source": [
    "# Move it to DBFS for Spark access\n",
    "dbutils.fs.mv(\"file:/dbfs/tmp/transactions/transactions.csv\", \"dbfs:/tmp/transactions/transactions.csv\")\n",
    "dbutils.fs.mv(\"file:/dbfs/tmp/transactions/CustomerImportance.csv\", \"dbfs:/tmp/transactions/CustomerImportance.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "144ed987-c0b7-485c-b31f-1b2551528b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n|step|     customer|age|gender|zipcodeOri|     merchant|zipMerchant|           category|amount|fraud|\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n|   0|'C1093826151'|'4'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'|  4.55|    0|\n|   0| 'C352968107'|'2'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 39.68|    0|\n|   0|'C2054744914'|'4'|   'F'|   '28007'|'M1823072687'|    '28007'|'es_transportation'| 26.89|    0|\n|   0|'C1760612790'|'3'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 17.25|    0|\n|   0| 'C757503768'|'5'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 35.72|    0|\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "#load transaction data\n",
    "df_txn = spark.read.csv(\"dbfs:/tmp/transactions/transactions.csv\", header=True, inferSchema=True)\n",
    "df_txn.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234b7acd-fbc8-4684-8fb5-9df551c64fb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+------+-------------------+-----+\n|       Source|       Target|Weight|          typeTrans|fraud|\n+-------------+-------------+------+-------------------+-----+\n|'C1093826151'| 'M348934600'|  4.55|'es_transportation'|    0|\n| 'C352968107'| 'M348934600'| 39.68|'es_transportation'|    0|\n|'C2054744914'|'M1823072687'| 26.89|'es_transportation'|    0|\n|'C1760612790'| 'M348934600'| 17.25|'es_transportation'|    0|\n| 'C757503768'| 'M348934600'| 35.72|'es_transportation'|    0|\n+-------------+-------------+------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "#load importance data\n",
    "df_importance = spark.read.csv(\"dbfs:/tmp/transactions/CustomerImportance.csv\", header=True, inferSchema=True)\n",
    "df_importance.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "603735e0-f5ed-48da-b457-9f085349470c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_txn = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"dbfs:/tmp/transactions/transactions.csv\")\n",
    "\n",
    "# Read CustomerImportance.csv\n",
    "df_importance_raw = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"dbfs:/tmp/transactions/CustomerImportance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96a6e591-1842-4834-a3d1-eb5002c9e707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_importance = df_importance_raw.selectExpr(\n",
    "    \"Source as customer\", \n",
    "    \"Target as merchant\", \n",
    "    \"typeTrans as category\", \n",
    "    \"Weight as weight\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74032c98-34ab-49bc-bbad-49c4be29caf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Testing the pattern identifications with the entire data to get an idea of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c9afad-a028-472a-821e-37e72dcc5c25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F, Window\n",
    "\n",
    "# Join transactions with importance weights\n",
    "# Join on customer, merchant, and transaction category\n",
    "df_joined = df_txn.join(df_importance, on=[\"customer\", \"merchant\", \"category\"], how=\"left\")\n",
    "\n",
    "\n",
    "# Total transactions per merchant\n",
    "merchant_txn_counts = df_txn.groupBy(\"merchant\").agg(F.count(\"*\").alias(\"merchant_txn_count\"))\n",
    "\n",
    "# Filter merchants with > 50k transactions\n",
    "large_merchants = merchant_txn_counts.filter(\"merchant_txn_count > 50000\")\n",
    "\n",
    "# Transactions for large merchants\n",
    "filtered_txns = df_joined.join(large_merchants, on=\"merchant\")\n",
    "\n",
    "# Transactions per customer per merchant\n",
    "customer_txn_count = filtered_txns.groupBy(\"merchant\", \"customer\").agg(\n",
    "    F.count(\"*\").alias(\"txn_count\"),\n",
    "    F.avg(\"weight\").alias(\"avg_weight\")\n",
    ")\n",
    "\n",
    "# Compute percentiles using approxQuantile (alternative: Window if needed)\n",
    "for merchant in large_merchants.select(\"merchant\").collect():\n",
    "    m = merchant[\"merchant\"]\n",
    "    df_m = customer_txn_count.filter(F.col(\"merchant\") == m)\n",
    "    txn_thresh = df_m.approxQuantile(\"txn_count\", [0.99], 0.01)[0]\n",
    "    weight_thresh = df_m.approxQuantile(\"avg_weight\", [0.01], 0.01)[0]\n",
    "    \n",
    "    df_detected = df_m.filter(\n",
    "        (F.col(\"txn_count\") >= txn_thresh) & (F.col(\"avg_weight\") <= weight_thresh)\n",
    "    ).withColumn(\"patternId\", F.lit(\"PatId1\")) \\\n",
    "     .withColumn(\"actionType\", F.lit(\"UPGRADE\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d49b46cf-02b3-47f8-a1b3-fa66237a8584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n|     merchant|merchant_txn_count|\n+-------------+------------------+\n| 'M348934600'|            205426|\n|'M1823072687'|            299693|\n+-------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "large_merchants.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4bd54db-580b-4177-be96-ff3ae9c97559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n|summary|         txn_count|        avg_amount|\n+-------+------------------+------------------+\n|  count|             47132|             47132|\n|   mean|12.616545022490028|106.61830155296524|\n| stddev| 29.13387597583581| 295.6577402990145|\n|    min|                 1|              0.01|\n|    max|               163|           8329.96|\n+-------+------------------+------------------+\n\nPatId2 count: 34\n+-------------+-------------+---------+------------------+---------+----------+-------------+-------------+\n|customer     |merchant     |txn_count|avg_amount        |patternId|actionType|customerName |merchantId   |\n+-------------+-------------+---------+------------------+---------+----------+-------------+-------------+\n|'C71938921'  |'M348934600' |112      |22.461785714285718|PatId2   |CHILD     |'C71938921'  |'M348934600' |\n|'C1799527037'|'M1823072687'|103      |22.75106796116505 |PatId2   |CHILD     |'C1799527037'|'M1823072687'|\n|'C1432312197'|'M348934600' |101      |22.729900990099008|PatId2   |CHILD     |'C1432312197'|'M348934600' |\n|'C245908628' |'M348934600' |125      |22.510800000000003|PatId2   |CHILD     |'C245908628' |'M348934600' |\n|'C1999090638'|'M348934600' |91       |20.77824175824176 |PatId2   |CHILD     |'C1999090638'|'M348934600' |\n|'C1214229415'|'M1823072687'|136      |22.573161764705883|PatId2   |CHILD     |'C1214229415'|'M1823072687'|\n|'C747353905' |'M1823072687'|89       |22.66876404494382 |PatId2   |CHILD     |'C747353905' |'M1823072687'|\n|'C474891377' |'M1823072687'|120      |22.757583333333333|PatId2   |CHILD     |'C474891377' |'M1823072687'|\n|'C1098443227'|'M1823072687'|80       |22.187625         |PatId2   |CHILD     |'C1098443227'|'M1823072687'|\n|'C1273110804'|'M1823072687'|86       |22.590232558139533|PatId2   |CHILD     |'C1273110804'|'M1823072687'|\n+-------------+-------------+---------+------------------+---------+----------+-------------+-------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    " from pyspark.sql.functions import col, count, avg, lit, countDistinct\n",
    "\n",
    "# Step 1: Group by customer and merchant\n",
    "cust_merchant_stats = df_txn.groupBy(\"customer\", \"merchant\").agg(\n",
    "    count(\"*\").alias(\"txn_count\"),\n",
    "    avg(\"amount\").alias(\"avg_amount\")\n",
    ")\n",
    "\n",
    "# Step 2: See distribution (optional)\n",
    "cust_merchant_stats.describe([\"txn_count\", \"avg_amount\"]).show()\n",
    "\n",
    "# Step 3: Apply filter for PatId2 logic\n",
    "pat2 = cust_merchant_stats.filter(\n",
    "    (col(\"txn_count\") >= 80) & (col(\"avg_amount\") < 23)\n",
    ").withColumn(\"patternId\", lit(\"PatId2\")) \\\n",
    " .withColumn(\"actionType\", lit(\"CHILD\")) \\\n",
    " .withColumn(\"customerName\", col(\"customer\")) \\\n",
    " .withColumn(\"merchantId\", col(\"merchant\"))\n",
    "\n",
    "# Step 4: Show results\n",
    "print(\"PatId2 count:\", pat2.count())\n",
    "pat2.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0014d7e0-90e3-4d5f-81de-d72fb8950b15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Clean gender column (strip single quotes)\n",
    "df_txn_clean = df_txn.withColumn(\"gender\", regexp_replace(\"gender\", \"'\", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c608b2b2-5fc5-48d6-843b-476c9d088098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----+\n|merchant     |F   |M   |\n+-------------+----+----+\n|'M348934600' |2147|1771|\n|'M1823072687'|1945|1617|\n|'M85975013'  |1870|1561|\n|'M855959430' |1594|1329|\n|'M1053599405'|1571|1248|\n|'M151143676' |1497|1226|\n|'M1946091778'|1426|1241|\n|'M209847108' |1183|974 |\n|'M1600850729'|1079|878 |\n|'M1913465890'|1062|844 |\n|'M349281107' |1040|840 |\n|'M480139044' |766 |556 |\n|'M840466850' |618 |507 |\n|'M1535107174'|598 |433 |\n|'M1198415165'|580 |429 |\n|'M78078399'  |552 |440 |\n|'M1649169323'|545 |406 |\n|'M980657600' |458 |336 |\n|'M1842530320'|396 |330 |\n|'M1400236507'|386 |293 |\n+-------------+----+----+\nonly showing top 20 rows\n\nPatId3 Count: 1\n+------------+---+---+---------+----------+------------+------------+\n|merchant    |F  |M  |patternId|actionType|customerName|merchantId  |\n+------------+---+---+---------+----------+------------+------------+\n|'M677738360'|173|174|PatId3   |DEI-NEEDED|            |'M677738360'|\n+------------+---+---+---------+----------+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Use cleaned version here:\n",
    "df_gender_base = df_txn_clean.select(\"merchant\", \"customer\", \"gender\").dropDuplicates()\n",
    "\n",
    "gender_counts = df_gender_base.groupBy(\"merchant\", \"gender\") \\\n",
    "    .agg(countDistinct(\"customer\").alias(\"cust_count\"))\n",
    "\n",
    "gender_pivot = gender_counts.groupBy(\"merchant\") \\\n",
    "    .pivot(\"gender\", [\"F\", \"M\"]).sum(\"cust_count\").na.fill(0)\n",
    "\n",
    "gender_pivot = gender_pivot.select(\n",
    "    col(\"merchant\"),\n",
    "    col(\"F\").cast(\"int\").alias(\"F\"),\n",
    "    col(\"M\").cast(\"int\").alias(\"M\")\n",
    ")\n",
    "\n",
    "gender_pivot.orderBy(col(\"F\").desc()).show(20, truncate=False)\n",
    "\n",
    "pat3 = gender_pivot.filter(\n",
    "    (col(\"F\") > 100) & (col(\"F\") < col(\"M\"))\n",
    ").withColumn(\"patternId\", lit(\"PatId3\")) \\\n",
    " .withColumn(\"actionType\", lit(\"DEI-NEEDED\")) \\\n",
    " .withColumn(\"customerName\", lit(\"\")) \\\n",
    " .withColumn(\"merchantId\", col(\"merchant\"))\n",
    "\n",
    "print(\"PatId3 Count:\", pat3.count())\n",
    "pat3.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5689898d-9906-4739-9252-3013ca72ddda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Total rows: 594643, Chunks: 60\n⏭️ Chunk 0 already exists. Skipping...\n⏭️ Chunk 1 already exists. Skipping...\n✅ Wrote chunk 2 (20000–30000)\n✅ Wrote chunk 3 (30000–40000)\n✅ Wrote chunk 4 (40000–50000)\n✅ Wrote chunk 5 (50000–60000)\n✅ Wrote chunk 6 (60000–70000)\n✅ Wrote chunk 7 (70000–80000)\n✅ Wrote chunk 8 (80000–90000)\n✅ Wrote chunk 9 (90000–100000)\n✅ Wrote chunk 10 (100000–110000)\n✅ Wrote chunk 11 (110000–120000)\n✅ Wrote chunk 12 (120000–130000)\n✅ Wrote chunk 13 (130000–140000)\n✅ Wrote chunk 14 (140000–150000)\n✅ Wrote chunk 15 (150000–160000)\n✅ Wrote chunk 16 (160000–170000)\n✅ Wrote chunk 17 (170000–180000)\n✅ Wrote chunk 18 (180000–190000)\n✅ Wrote chunk 19 (190000–200000)\n✅ Wrote chunk 20 (200000–210000)\n✅ Wrote chunk 21 (210000–220000)\n✅ Wrote chunk 22 (220000–230000)\n✅ Wrote chunk 23 (230000–240000)\n✅ Wrote chunk 24 (240000–250000)\n✅ Wrote chunk 25 (250000–260000)\n✅ Wrote chunk 26 (260000–270000)\n✅ Wrote chunk 27 (270000–280000)\n✅ Wrote chunk 28 (280000–290000)\n✅ Wrote chunk 29 (290000–300000)\n✅ Wrote chunk 30 (300000–310000)\n✅ Wrote chunk 31 (310000–320000)\n✅ Wrote chunk 32 (320000–330000)\n✅ Wrote chunk 33 (330000–340000)\n✅ Wrote chunk 34 (340000–350000)\n✅ Wrote chunk 35 (350000–360000)\n✅ Wrote chunk 36 (360000–370000)\n✅ Wrote chunk 37 (370000–380000)\n✅ Wrote chunk 38 (380000–390000)\n✅ Wrote chunk 39 (390000–400000)\n✅ Wrote chunk 40 (400000–410000)\n✅ Wrote chunk 41 (410000–420000)\n✅ Wrote chunk 42 (420000–430000)\n✅ Wrote chunk 43 (430000–440000)\n✅ Wrote chunk 44 (440000–450000)\n✅ Wrote chunk 45 (450000–460000)\n✅ Wrote chunk 46 (460000–470000)\n✅ Wrote chunk 47 (470000–480000)\n✅ Wrote chunk 48 (480000–490000)\n✅ Wrote chunk 49 (490000–500000)\n✅ Wrote chunk 50 (500000–510000)\n✅ Wrote chunk 51 (510000–520000)\n✅ Wrote chunk 52 (520000–530000)\n✅ Wrote chunk 53 (530000–540000)\n✅ Wrote chunk 54 (540000–550000)\n✅ Wrote chunk 55 (550000–560000)\n✅ Wrote chunk 56 (560000–570000)\n✅ Wrote chunk 57 (570000–580000)\n✅ Wrote chunk 58 (580000–590000)\n✅ Wrote chunk 59 (590000–600000)\n🎉 Stream simulation complete.\n"
     ]
    }
   ],
   "source": [
    "# Mechanism X - Enhanced with PostgreSQL Tracking\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "import psycopg2\n",
    "\n",
    "# Configuration\n",
    "chunk_size = 10000\n",
    "bucket_name = \"bucket-aswanthlal-20250609123312\"\n",
    "output_dir = f\"s3a://{bucket_name}/stream_chunks\"\n",
    "access_key = \"AKIAU76LUFEGVNYPMUVE\"\n",
    "secret_key = \"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\"\n",
    "pg_conn_params = {\n",
    "    'host': 'txndb.cvskc4cqgii5.ap-south-1.rds.amazonaws.com',\n",
    "    'port': 5432,\n",
    "    'dbname': 'postgres',\n",
    "    'user': 'txn_postgres',\n",
    "    'password': 'rootroot'\n",
    "}\n",
    "\n",
    "# Step 1: Setup S3 credentials\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.ap-south-1.amazonaws.com\")\n",
    "\n",
    "# Step 2: Add row index to DataFrame\n",
    "df_txn_indexed = df_txn.withColumn(\n",
    "    \"row_id\", row_number().over(Window.orderBy(\"customer\", \"merchant\", \"amount\"))\n",
    ")\n",
    "total_rows = df_txn_indexed.count()\n",
    "num_chunks = (total_rows + chunk_size - 1) // chunk_size\n",
    "print(f\"🔢 Total rows: {total_rows}, Chunks: {num_chunks}\")\n",
    "\n",
    "# Step 3: PostgreSQL setup\n",
    "create_table_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS processed_chunks (\n",
    "    chunk_index INT PRIMARY KEY,\n",
    "    written_at TIMESTAMP DEFAULT now(),\n",
    "    s3_path TEXT NOT NULL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "insert_chunk_sql = \"\"\"\n",
    "INSERT INTO processed_chunks (chunk_index, s3_path)\n",
    "VALUES (%s, %s)\n",
    "ON CONFLICT (chunk_index) DO NOTHING;\n",
    "\"\"\"\n",
    "\n",
    "select_chunks_sql = \"SELECT chunk_index FROM processed_chunks;\"\n",
    "\n",
    "# Step 4: Connect to PostgreSQL and get existing chunks\n",
    "conn = psycopg2.connect(**pg_conn_params)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(create_table_sql)\n",
    "conn.commit()\n",
    "cursor.execute(select_chunks_sql)\n",
    "existing_indices = {row[0] for row in cursor.fetchall()}\n",
    "\n",
    "# Step 5: Loop through chunks\n",
    "for chunk_index in range(num_chunks):\n",
    "    if chunk_index in existing_indices:\n",
    "        print(f\"⏭️ Chunk {chunk_index} already exists. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    start_idx = chunk_index * chunk_size\n",
    "    end_idx = start_idx + chunk_size\n",
    "\n",
    "    chunk_df = df_txn_indexed.filter(\n",
    "        (col(\"row_id\") > start_idx) & (col(\"row_id\") <= end_idx)\n",
    "    ).drop(\"row_id\")\n",
    "\n",
    "    if chunk_df.rdd.isEmpty():\n",
    "        print(f\"⚠️ Empty chunk at index {chunk_index}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    timestamp = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    chunk_path = f\"{output_dir}/transactions_chunk_{chunk_index}_{timestamp}.csv\"\n",
    "    chunk_df.write.mode(\"overwrite\").option(\"header\", True).csv(chunk_path)\n",
    "    print(f\"✅ Wrote chunk {chunk_index} ({start_idx}–{end_idx})\")\n",
    "\n",
    "    # Log to PostgreSQL\n",
    "    cursor.execute(insert_chunk_sql, (chunk_index, chunk_path))\n",
    "    conn.commit()\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"🎉 Stream simulation complete.\")\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e9cbe96-1c55-45d9-b0cb-ede63b9ea213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_dir = \"s3a://bucket-aswanthlal-20250609123312/stream_chunks\"\n",
    "files = dbutils.fs.ls(output_dir)\n",
    "display(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adf38e4d-3923-4cd5-ad4c-20cd6b666369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Calculate total amount\n",
    "pat2_to_write = pat2 \\\n",
    "    .withColumn(\"total_amount_sum\", col(\"avg_amount\") * col(\"txn_count\")) \\\n",
    "    .select(\n",
    "        col(\"customer\"),\n",
    "        col(\"merchant\"),\n",
    "        col(\"txn_count\").alias(\"total_txn_count\"),\n",
    "        col(\"total_amount_sum\")\n",
    "    ) \\\n",
    "    .limit(10)  # Write only 10 rows for initial test\n",
    "\n",
    "# PostgreSQL JDBC config\n",
    "pg_url = \"jdbc:postgresql://txndb.cvskc4cqgii5.ap-south-1.rds.amazonaws.com:5432/postgres\"\n",
    "pg_properties = {\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"adminadmin\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Write to table\n",
    "pat2_to_write.write \\\n",
    "    .jdbc(url=pg_url, table=\"customer_merchant_cumulative_agg\", mode=\"append\", properties=pg_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1595f769-bc82-4944-b1ac-6c8298eb50dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing: transactions_chunk_10_20250611114916.csv\n✅ Finished processing transactions_chunk_10_20250611114916.csv\n🚀 Processing: transactions_chunk_11_20250611114933.csv\n✅ Finished processing transactions_chunk_11_20250611114933.csv\n🚀 Processing: transactions_chunk_12_20250611114951.csv\n✅ Finished processing transactions_chunk_12_20250611114951.csv\n🚀 Processing: transactions_chunk_13_20250611115008.csv\n✅ Finished processing transactions_chunk_13_20250611115008.csv\n🚀 Processing: transactions_chunk_14_20250611115026.csv\n✅ Finished processing transactions_chunk_14_20250611115026.csv\n🚀 Processing: transactions_chunk_15_20250611115044.csv\n✅ Finished processing transactions_chunk_15_20250611115044.csv\n🚀 Processing: transactions_chunk_16_20250611115100.csv\n✅ Finished processing transactions_chunk_16_20250611115100.csv\n🚀 Processing: transactions_chunk_17_20250611115117.csv\n✅ Finished processing transactions_chunk_17_20250611115117.csv\n🚀 Processing: transactions_chunk_18_20250611115135.csv\n✅ Finished processing transactions_chunk_18_20250611115135.csv\n🚀 Processing: transactions_chunk_19_20250611115154.csv\n✅ Finished processing transactions_chunk_19_20250611115154.csv\n🚀 Processing: transactions_chunk_20_20250611115211.csv\n✅ Finished processing transactions_chunk_20_20250611115211.csv\n🚀 Processing: transactions_chunk_21_20250611115230.csv\n✅ Finished processing transactions_chunk_21_20250611115230.csv\n🚀 Processing: transactions_chunk_22_20250611115250.csv\n✅ Finished processing transactions_chunk_22_20250611115250.csv\n🚀 Processing: transactions_chunk_23_20250611115308.csv\n✅ Finished processing transactions_chunk_23_20250611115308.csv\n🚀 Processing: transactions_chunk_24_20250611115328.csv\n✅ Finished processing transactions_chunk_24_20250611115328.csv\n🚀 Processing: transactions_chunk_25_20250611115347.csv\n✅ Finished processing transactions_chunk_25_20250611115347.csv\n🚀 Processing: transactions_chunk_26_20250611115407.csv\n✅ Finished processing transactions_chunk_26_20250611115407.csv\n🚀 Processing: transactions_chunk_27_20250611115426.csv\n✅ Finished processing transactions_chunk_27_20250611115426.csv\n🚀 Processing: transactions_chunk_28_20250611115444.csv\n✅ Finished processing transactions_chunk_28_20250611115444.csv\n🚀 Processing: transactions_chunk_29_20250611115504.csv\n✅ Finished processing transactions_chunk_29_20250611115504.csv\n🚀 Processing: transactions_chunk_2_20250611114656.csv\n✅ Finished processing transactions_chunk_2_20250611114656.csv\n🚀 Processing: transactions_chunk_30_20250611115524.csv\n✅ Finished processing transactions_chunk_30_20250611115524.csv\n🚀 Processing: transactions_chunk_31_20250611115542.csv\n✅ Finished processing transactions_chunk_31_20250611115542.csv\n🚀 Processing: transactions_chunk_32_20250611115601.csv\n✅ Finished processing transactions_chunk_32_20250611115601.csv\n🚀 Processing: transactions_chunk_33_20250611115620.csv\n✅ Finished processing transactions_chunk_33_20250611115620.csv\n🚀 Processing: transactions_chunk_34_20250611115641.csv\n✅ Finished processing transactions_chunk_34_20250611115641.csv\n🚀 Processing: transactions_chunk_35_20250611115700.csv\n✅ Finished processing transactions_chunk_35_20250611115700.csv\n🚀 Processing: transactions_chunk_36_20250611115718.csv\n✅ Finished processing transactions_chunk_36_20250611115718.csv\n🚀 Processing: transactions_chunk_37_20250611115737.csv\n✅ Finished processing transactions_chunk_37_20250611115737.csv\n🚀 Processing: transactions_chunk_38_20250611115756.csv\n✅ Finished processing transactions_chunk_38_20250611115756.csv\n🚀 Processing: transactions_chunk_39_20250611115814.csv\n✅ Finished processing transactions_chunk_39_20250611115814.csv\n🚀 Processing: transactions_chunk_3_20250611114713.csv\n✅ Finished processing transactions_chunk_3_20250611114713.csv\n🚀 Processing: transactions_chunk_40_20250611115832.csv\n✅ Finished processing transactions_chunk_40_20250611115832.csv\n🚀 Processing: transactions_chunk_41_20250611115851.csv\n✅ Finished processing transactions_chunk_41_20250611115851.csv\n🚀 Processing: transactions_chunk_42_20250611115911.csv\n✅ Finished processing transactions_chunk_42_20250611115911.csv\n🚀 Processing: transactions_chunk_43_20250611115930.csv\n✅ Finished processing transactions_chunk_43_20250611115930.csv\n🚀 Processing: transactions_chunk_44_20250611115949.csv\n✅ Finished processing transactions_chunk_44_20250611115949.csv\n🚀 Processing: transactions_chunk_45_20250611120008.csv\n✅ Finished processing transactions_chunk_45_20250611120008.csv\n🚀 Processing: transactions_chunk_46_20250611120027.csv\n✅ Finished processing transactions_chunk_46_20250611120027.csv\n🚀 Processing: transactions_chunk_47_20250611120044.csv\n✅ Finished processing transactions_chunk_47_20250611120044.csv\n🚀 Processing: transactions_chunk_48_20250611120101.csv\n✅ Finished processing transactions_chunk_48_20250611120101.csv\n🚀 Processing: transactions_chunk_49_20250611120119.csv\n✅ Finished processing transactions_chunk_49_20250611120119.csv\n🚀 Processing: transactions_chunk_4_20250611114730.csv\n✅ Finished processing transactions_chunk_4_20250611114730.csv\n🚀 Processing: transactions_chunk_50_20250611120138.csv\n✅ Finished processing transactions_chunk_50_20250611120138.csv\n🚀 Processing: transactions_chunk_51_20250611120156.csv\n✅ Finished processing transactions_chunk_51_20250611120156.csv\n🚀 Processing: transactions_chunk_52_20250611120214.csv\n✅ Finished processing transactions_chunk_52_20250611120214.csv\n🚀 Processing: transactions_chunk_53_20250611120231.csv\n✅ Finished processing transactions_chunk_53_20250611120231.csv\n🚀 Processing: transactions_chunk_54_20250611120251.csv\n✅ Finished processing transactions_chunk_54_20250611120251.csv\n🚀 Processing: transactions_chunk_55_20250611120308.csv\n✅ Finished processing transactions_chunk_55_20250611120308.csv\n🚀 Processing: transactions_chunk_56_20250611120327.csv\n✅ Finished processing transactions_chunk_56_20250611120327.csv\n🚀 Processing: transactions_chunk_57_20250611120344.csv\n✅ Finished processing transactions_chunk_57_20250611120344.csv\n🚀 Processing: transactions_chunk_58_20250611120402.csv\n✅ Finished processing transactions_chunk_58_20250611120402.csv\n🚀 Processing: transactions_chunk_59_20250611120420.csv\n✅ Finished processing transactions_chunk_59_20250611120420.csv\n🚀 Processing: transactions_chunk_5_20250611114748.csv\n✅ Finished processing transactions_chunk_5_20250611114748.csv\n🚀 Processing: transactions_chunk_6_20250611114805.csv\n✅ Finished processing transactions_chunk_6_20250611114805.csv\n🚀 Processing: transactions_chunk_7_20250611114823.csv\n✅ Finished processing transactions_chunk_7_20250611114823.csv\n🚀 Processing: transactions_chunk_8_20250611114841.csv\n✅ Finished processing transactions_chunk_8_20250611114841.csv\n🚀 Processing: transactions_chunk_9_20250611114858.csv\n✅ Finished processing transactions_chunk_9_20250611114858.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import psycopg2\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ---- Configuration ----\n",
    "bucket = \"bucket-aswanthlal-20250609123312\"\n",
    "chunk_dir = f\"s3a://{bucket}/stream_chunks\"\n",
    "jdbc_url = \"jdbc:postgresql://txndb.cvskc4cqgii5.ap-south-1.rds.amazonaws.com:5432/postgres\"\n",
    "jdbc_props = {\n",
    "    \"user\": \"txn_postgres\",\n",
    "    \"password\": \"rootroot\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "processed_table = \"processed_chunks\"\n",
    "pattern_output_table = \"detected_patterns\"\n",
    "\n",
    "# ---- Define schema for CSV chunks ----\n",
    "chunk_schema = StructType([\n",
    "    StructField(\"step\", IntegerType(), True),\n",
    "    StructField(\"customer\", StringType(), True),\n",
    "    StructField(\"age\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"zipcodeOri\", StringType(), True),\n",
    "    StructField(\"merchant\", StringType(), True),\n",
    "    StructField(\"zipMerchant\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"fraud\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# ---- Helper: Get processed chunks from PostgreSQL ----\n",
    "def get_processed_chunks():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"txndb.cvskc4cqgii5.ap-south-1.rds.amazonaws.com\",\n",
    "        dbname=\"postgres\",\n",
    "        user=\"txn_postgres\",\n",
    "        password=\"rootroot\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT chunk_name FROM processed_chunks\")\n",
    "    rows = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return {row[0] for row in rows}\n",
    "\n",
    "# ---- Load and rename customer importance DataFrame ----\n",
    "df_importance = spark.read.csv(\"dbfs:/tmp/transactions/CustomerImportance.csv\", header=True, inferSchema=True)\n",
    "df_importance = df_importance.withColumnRenamed(\"Source\", \"customer\") \\\n",
    "                             .withColumnRenamed(\"Target\", \"merchant\") \\\n",
    "                             .withColumnRenamed(\"typeTrans\", \"category\")\n",
    "\n",
    "# ---- Pattern Detection Functions ----\n",
    "def detect_patid1(df_txn: DataFrame, df_importance: DataFrame) -> DataFrame:\n",
    "    df_joined = df_txn.alias(\"tx\").join(\n",
    "        df_importance.alias(\"imp\"),\n",
    "        (F.col(\"tx.customer\") == F.col(\"imp.customer\")) &\n",
    "        (F.col(\"tx.merchant\") == F.col(\"imp.merchant\")),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    merchant_txn_counts = df_txn.groupBy(\"merchant\").count().withColumnRenamed(\"count\", \"total_txns\")\n",
    "    eligible_merchants = merchant_txn_counts.filter(F.col(\"total_txns\") > 50000)\n",
    "\n",
    "    df_joined = df_joined.join(\n",
    "        eligible_merchants.alias(\"em\"),\n",
    "        F.col(\"tx.merchant\") == F.col(\"em.merchant\"),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    agg_df = df_joined.groupBy(\n",
    "        F.col(\"tx.customer\").alias(\"customer\"),\n",
    "        F.col(\"tx.merchant\").alias(\"merchant\")\n",
    "    ).agg(\n",
    "        F.count(\"*\").alias(\"txn_count\"),\n",
    "        F.avg(F.col(\"imp.Weight\")).alias(\"avg_weight\")\n",
    "    )\n",
    "\n",
    "    window_spec = Window.partitionBy(\"merchant\").orderBy(F.col(\"txn_count\").desc())\n",
    "    window_spec_weight = Window.partitionBy(\"merchant\").orderBy(F.col(\"avg_weight\").asc())\n",
    "\n",
    "    ranked_df = agg_df.withColumn(\"txn_rank\", F.percent_rank().over(window_spec)) \\\n",
    "                      .withColumn(\"weight_rank\", F.percent_rank().over(window_spec_weight))\n",
    "\n",
    "    patid1 = ranked_df.filter(\n",
    "        (F.col(\"txn_rank\") <= 0.01) & (F.col(\"weight_rank\") <= 0.01)\n",
    "    ).withColumn(\"patternId\", F.lit(\"PatId1\")) \\\n",
    "     .withColumn(\"actionType\", F.lit(\"UPGRADE\"))\n",
    "\n",
    "    return patid1\n",
    "\n",
    "def detect_patid2(df_txn: DataFrame) -> DataFrame:\n",
    "    agg_df = df_txn.groupBy(\"customer\", \"merchant\").agg(\n",
    "        F.count(\"*\").alias(\"txn_count\"),\n",
    "        F.avg(\"amount\").alias(\"avg_amount\")\n",
    "    )\n",
    "\n",
    "    patid2 = agg_df.filter(\n",
    "        (F.col(\"txn_count\") >= 80) & (F.col(\"avg_amount\") < 23)\n",
    "    ).withColumn(\"patternId\", F.lit(\"PatId2\")) \\\n",
    "     .withColumn(\"actionType\", F.lit(\"CHILD\"))\n",
    "\n",
    "    return patid2\n",
    "\n",
    "def detect_patid3(df_txn: DataFrame) -> DataFrame:\n",
    "    df_gender_base = df_txn.select(\"merchant\", \"customer\", \"gender\").dropDuplicates()\n",
    "\n",
    "    gender_counts = df_gender_base.groupBy(\"merchant\", \"gender\").agg(\n",
    "        F.countDistinct(\"customer\").alias(\"cust_count\")\n",
    "    ).groupBy(\"merchant\").pivot(\"gender\", [\"F\", \"M\"]).sum(\"cust_count\").na.fill(0)\n",
    "\n",
    "    patid3 = gender_counts.filter(\n",
    "        (F.col(\"F\") > 100) & (F.col(\"F\") < F.col(\"M\"))\n",
    "    ).withColumn(\"patternId\", F.lit(\"PatId3\")) \\\n",
    "     .withColumn(\"actionType\", F.lit(\"DEI-NEEDED\")) \\\n",
    "     .withColumn(\"customerName\", F.lit(\"\")) \\\n",
    "     .withColumn(\"merchantId\", F.col(\"merchant\"))\n",
    "\n",
    "    return patid3\n",
    "\n",
    "\n",
    "# ---- Start processing chunks ----\n",
    "processed_chunks = get_processed_chunks()\n",
    "all_files = [f.path for f in dbutils.fs.ls(chunk_dir) if f.path.endswith(\".csv/\")]\n",
    "\n",
    "for file_path in all_files:\n",
    "    chunk_name = os.path.basename(file_path.rstrip(\"/\"))\n",
    "    if chunk_name in processed_chunks:\n",
    "        print(f\"✅ Already processed: {chunk_name}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"🚀 Processing: {chunk_name}\")\n",
    "    df_chunk = spark.read.schema(chunk_schema).option(\"header\", True).csv(file_path)\n",
    "    df_chunk = df_chunk.withColumn(\"chunk_name\", lit(chunk_name))\n",
    "\n",
    "    # Pattern detection\n",
    "    patid1_df = detect_patid1(df_chunk, df_importance)\n",
    "    patid2_df = detect_patid2(df_chunk)\n",
    "    patid3_df = detect_patid3(df_chunk)\n",
    "\n",
    "    # Harmonize schemas\n",
    "    patid1_df = patid1_df.withColumn(\"avg_amount\", lit(None).cast(\"double\")) \\\n",
    "                         .withColumn(\"F\", lit(None).cast(\"int\")) \\\n",
    "                         .withColumn(\"M\", lit(None).cast(\"int\"))\n",
    "\n",
    "    patid2_df = patid2_df.withColumn(\"avg_weight\", lit(None).cast(\"double\")) \\\n",
    "                         .withColumn(\"txn_rank\", lit(None).cast(\"double\")) \\\n",
    "                         .withColumn(\"weight_rank\", lit(None).cast(\"double\")) \\\n",
    "                         .withColumn(\"F\", lit(None).cast(\"int\")) \\\n",
    "                         .withColumn(\"M\", lit(None).cast(\"int\"))\n",
    "\n",
    "    patid3_df = patid3_df.withColumn(\"avg_weight\", lit(None).cast(\"double\")) \\\n",
    "                         .withColumn(\"txn_count\", lit(None).cast(\"int\")) \\\n",
    "                         .withColumn(\"avg_amount\", lit(None).cast(\"double\")) \\\n",
    "                         .withColumn(\"txn_rank\", lit(None).cast(\"double\")) \\\n",
    "                         .withColumn(\"weight_rank\", lit(None).cast(\"double\")) \\\n",
    "                         .withColumn(\"customer\", lit(None).cast(\"string\"))\n",
    "\n",
    "    # Union all\n",
    "    result_df = patid1_df.unionByName(patid2_df).unionByName(patid3_df)\n",
    "    result_df = result_df.withColumn(\"chunk_name\", lit(chunk_name)) \\\n",
    "                         .withColumn(\"processed_at\", lit(datetime.utcnow().isoformat()))\n",
    "\n",
    "from pytz import timezone\n",
    "\n",
    "# Define output directory for patterns\n",
    "output_dir = f\"s3a://{bucket}/pattern_detections\"\n",
    "y_start_time = datetime.now(timezone(\"Asia/Kolkata\")).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Format the output schema\n",
    "final_df = result_df.withColumn(\"YStartTime\", F.lit(y_start_time)) \\\n",
    "    .withColumn(\"detectionTime\", F.current_timestamp()) \\\n",
    "    .withColumn(\"detectionTime\", F.date_format(F.from_utc_timestamp(\"detectionTime\", \"Asia/Kolkata\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"customerName\", F.coalesce(F.col(\"customer\"), F.lit(\"\"))) \\\n",
    "    .withColumn(\"merchantId\", F.coalesce(F.col(\"merchant\"), F.lit(\"\"))) \\\n",
    "    .select(\"YStartTime\", \"detectionTime\", \"patternId\", \"actionType\", \"customerName\", \"merchantId\")\n",
    "\n",
    "# Break into batches of 50\n",
    "final_rows = final_df.collect()\n",
    "for i in range(0, len(final_rows), 50):\n",
    "    batch = final_rows[i:i+50]\n",
    "    if not batch:\n",
    "        continue\n",
    "    batch_df = spark.createDataFrame(batch, final_df.schema)\n",
    "    output_file = f\"{output_dir}/pattern_output_{chunk_name}_batch{i//50 + 1}.csv\"\n",
    "    batch_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(output_file)\n",
    "\n",
    "# ✅ Also update processed_chunks table as before\n",
    "processed_df = spark.createDataFrame([(chunk_name,)], [\"chunk_name\"])\n",
    "processed_df.write.jdbc(url=jdbc_url, table=processed_table, mode=\"append\", properties=jdbc_props)\n",
    "\n",
    "print(f\"✅ Finished processing {chunk_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mechanism_x",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}